\chapter{Deep Learning Overview}\label{chap:dlo}

\localtableofcontents

\section{Introduction}

Deep learning is a subfield of machine learning that focuses on the study of
\acp{DNN}. \acp{DNN} are a family of machine learning algorithms that have their
roots in \acp{ANN} which aim to learn a data representation from unstructured
data such as raw images \cite{DBLP:conf/nips/KrizhevskySH12}, text
\cite{DBLP:conf/emnlp/BudzianowskiV19} or audio
\cite{DBLP:journals/corr/HannunCCCDEPSSCN14}, in an end-to-end fashion.
\acp{ANN} were initially conceptualised based on the understanding of biological
neural networks present in the brain
\cite{mcculloch1943logical,hebb2005organization}.
\citeauthor{rosenblatt1958perceptron} proposed in
\cite{rosenblatt1958perceptron} a theoretical model of a neuron, denoted the
\emph{perceptron}, which was capable of learning a linear decision boundary. The
perceptron model was later extended to multiple layers of neurons, giving rise
to the \ac{MLP} \cite{rosenblatt1961principles,rumelhart1986learning}. A
\acl{MLP} is a type of artificial neural network that extends the concept of a
single-layer perceptron by including one or more hidden layers of neurons
connected upstream to an input layer and downstream to an output layer. Each
layer is fully connected to the next, allowing the model to learn and represent
more complex, non-linear relationships in the input data. Although able to learn
more complex boundaries than the perceptron, the \ac{MLP} is still limited by
its depth. The next advance came from the stacking of multiple layers, leading
to \aclp{DNN}.\\

In the context of \acp{DNN}, the term \emph{deep} denotes the stacking of many
layers within a neural network. The concept of \acp{DNN} is based on the idea
that the depth and the numerous layers can help in learning features at various
levels of abstraction, enabling the network to learn complex hierarchical
patterns. For instance, in the context of image recognition, lower layers may
learn local features like edges and textures, while deeper layers might learn to
identify more abstract concepts like shapes or objects.\\

The rise of \acp{DNN} was made possible by several factors. On the one hand the
increase in computational power, and in particular the use of \acp{GPU}, which
made the training of deep networks feasible. Indeed, AlexNet, the first \ac{CNN}
to win the ImageNet Large Scale Visual Recognition Challenge
\cite{DBLP:conf/nips/KrizhevskySH12}, was trained on two \acp{GPU} in parallel
to accelerate computations. Nowadays, the use of \acp{GPU} or dedicated hardware
such as \acp{TPU} is ubiquitous and supported by all the major deep learning
frameworks
\cite{DBLP:journals/corr/AbadiABBCCCDDDG16,DBLP:conf/nips/PaszkeGMLBCKLGA19}.On
the other hand, the availability of large-scale datasets such as ImageNet
\cite{deng2009imagenet} allowed to train or pre-train deep networks with
millions of parameters without overfitting.\\

This chapter aims to give an overview of the different neural network
architectures, building blocks, training techniques and datasets that are widely
used in Deep Learning for computer vision as well as our experiments.
\Cref{sec:dlo:early_architectures} introduced the early neural network
architectures, namely the perceptron and the \ac{MLP}. \Cref{sec:dlo:training}
focuses on the functional definition of a neural network and its training.
\Cref{sec:dlo:cnn} presents the building blocks and architectures of various
\acp{CNN} for computer vision, and finally, \Cref{sec:dlo:datasets} gives an
overview of the datasets used in our experiments.
% TODO: rafiner cette partie

% In the context of \acp{DNN}, the term \emph{deep} denotes the stacking of many
% layers within a neural network. A \ac{DNN} consists of layers, each layer is an
% operation that takes information from the previous layers, processes it and then
% passes it on through an activation function
% \cite{glorot2011deep,DBLP:journals/pieee/LeCunBBH98,klambauer2017self} that
% introduce non linearity. Finally, this output is sent to the next layer. Among
% the most fundamental types of these layers are \emph{Convolutional} and
% \emph{Dense} (or fully connected) layers. \\

\section{Early architectures}\label{sec:dlo:early_architectures}

In this section, we present the perceptron \cite{rosenblatt1958perceptron} and
then the \acl{MLP} \cite{rosenblatt1961principles,rumelhart1986learning}. Both
are the two founding neural network architectures that led to the development of
\aclp{DNN}.

\subsection{Perceptron}\label{sec:dlo:perceptron}

The \emph{perceptron} is a model of artificial neuron, capable of learning a
linear decision boundary. It was proposed by
\citeauthor{rosenblatt1958perceptron} in 1958 \cite{rosenblatt1958perceptron}
and conceptualised based on the understanding of biological neural networks
present in the brain \cite{mcculloch1943logical,hebb2005organization}. The
perceptron is composed of inputs that are weighted and summed before being
passed through an nonlinear function refered to as an activation function. The
conceptrual representation of the perceptrion is displayed in
\cref{fig:dlo:perceptron} and its mathematical formulation is defined in
\cref{eqn:dlo:perceptron}: \\
% and can be express in vector form as written in \cref{eqn:dlo:perceptron_vector}.\\

\begin{equation}
  \label{eqn:dlo:perceptron}
\hat{y} = g(\sum_{i=1}^{n} w_i \cdot x_i + b)
\end{equation} \\


% \begin{equation}
%   \label{eqn:dlo:perceptron_vector}
%   \hat{y} = g(\mathbf{w}^T \mathbf{x} + b)
% \end{equation}\\

\noindent where $x_i$ is the $i$th input, $w_i$ is the weight associated with
the $i$th input,$n$ is the number of inputs, $b$ is the bias, $g$ is the
activation function, and $\hat{y}$ is the perceptron's output. This formulation
can also be written in vector form as in \cref{eqn:dlo:perceptron_vector}: \\

\begin{equation}
  \label{eqn:dlo:perceptron_vector}
  \hat{y} = g(\mathbf{w}^T \mathbf{x} + b)
\end{equation}\\

\noindent where $\mathbf{x}$ is the vector of inputs and $\mathbf{w}$ is the
vector of weights. The activation function is typically a nonlinear function,
such as the sigmoid function or the hyperbolic tangent function. Due to its
shallow architecture, the perceptron cannot learn complex decision boundaries.
Nevertheless, it is possible to stack several perceptrons to learn nonlinear
decision boundaries, leading to a \acl{MLP}.\\

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{chapter_dlo/assets/perceptron_scheme.pdf}
  \caption{Conceptual scheme of the \emph{perceptron}. Each input $x_i$ is multiplied
  by its associated weight $w_i$ and summed to the other weighted inputs. The
  bias $b$ is added to the sum and the result is passed through an activation
  function $g$ to produce the output $\hat{y}$.}
  \label{fig:dlo:perceptron}
\end{figure}

\subsection{Multilayer Perceptron}\label{sec:dlo:mlp}


The \acf{MLP} is an extension of the perceptron model, comprising multiple
layers of perceptrons, also referred to as neurons \cite{rumelhart1986learning}.
A \ac{MLP} with one hidden layer is represented in \cref{fig:dlo:mlp}. In the
latter, the circles represent the neurons, and the connections between
representing weights are materialised by lines. The \ac{MLP} is the simplest
type of feedforward \ac{ANN}. Feedforward refers to the fact that the
connections between neurons in the \ac{MLP} form a \acf{DAG}, where the outputs
of the neurons from one layer are passed to the next, with no backward
connections or feedback. Using the same notations as in
\cref{eqn:dlo:perceptron_vector}, the vector form of the \ac{MLP} displayed in
\cref{fig:dlo:mlp} can be written as in \cref{eqn:dlo:mlp}, where the subscript
of activation functions $g_i$, weight matrices $\mathbf{w}_i$ and bias vectors
$\mathbf{b}_i$ denotes their belonging to the $i$th layer.\\

\begin{equation}
  \label{eqn:dlo:mlp}
  \hat{\mathbf{y}} = g_2(\mathbf{w}_2^T \cdot  g_1(\mathbf{w}_2^T \cdot \mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2)
\end{equation}\\

Each layer of the \ac{MLP} being fully connected to the next one, it enables the
\ac{MLP} to handle problems that the perceptron cannot solve, such as problems
requiring nonlinear decision boundaries. Furthermore,
\citeauthor{cybenko1989approximation} proved in \cite{cybenko1989approximation}
that an \ac{MLP} can approximate continuous functions on compact subsets of
$\mathbb{R}^n$. This result is known as the \emph{Universal Approximation
Theorem}. Before the emergence of Deep Learning, \acp{MLP} have been applied to
various domains, including voice recognition, image recognition, and machine
translation \cite{wasserman1988neural}.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{chapter_dlo/assets/mlp_scheme.pdf}
  \caption{Conceptual scheme of a \ac{MLP} with one hidden layer. Each circle
  represent a neuron and each line a connection associated to a weight.}
  \label{fig:dlo:mlp}
\end{figure}

\section{Neural Network Training}\label{sec:dlo:training}

Neural Network Training revolves around the optimisation of a mapping function
which learns to predict outcomes based on input data by adjusting its internal
parameters, also refered to as weights. It involves iteratively tuning these
weights so that the discrepancy between the model's predicted output and the
actual output is minimised. The tuning of the weiths rely on gradient-based
methods %TODO: finir ce paragraphe.

\subsection{Functional definition}

Neural networks can be defined as a mapping function from an input space
$\mathcal{X}$ to an ouput space $\mathcal{Y}$. This mapping function $f$ is
parametrised by a set of parameters $\theta$, often called \emph{weights}. The
training of a neural network consists in tuning the parameters $\theta$ so that,
given an input $X_i$, the mapping function $f$ output, denoted $\hat{y}_i$, is
as close as possible to the associated true output $y_i$. This training is done
iteratively by using example pairs $(X, y) \in \mathcal{X} \times \mathcal{Y}$,
where $X\in\mathcal{X}$ is the input and $y\in\mathcal{Y}$ is the output. In the
context of image classification, $X$ is an image and $y$ is a label that
indicates the class of the associated image. A functional representation of a
neural network is given in \cref{eqn:dlo:nn_functional_definition}, where $f$ is
the neural network, $\theta$ is the set of parameters of the network,
$X\in\mathcal{X}$ is the input given to the neural network and $\hat{y}$ is the
output.\\

\begin{equation}
  \label{eqn:dlo:nn_functional_definition}
  % \centering
  \begingroup
  \setlength\arraycolsep{0pt}
  f \colon\begin{array}[t]{c >{{}}c<{{}} l}
    \mathcal{X} & \to     & \mathcal{Y} \\
    X                     & \mapsto & f(X, \theta) = \hat{y}
  \end{array}
  \endgroup
\end{equation}\\


Considering image classification,  the output $\hat{y}$ is a probability vector
where the largest coefficient is the one whose index corresponds to the
predicted class of the input image, often called a label. This vector is
generally converted into a one-hot vector, where the only non-zero coefficient is
at the index of the predicted class. The true label $y$, referred to as the
ground truth is either the class index and $y\in\llbracket0;n-1\rrbracket$,
where $n$ is the number of classes considered. The ground truth can also be
converted in a one-hot vector.\\

% In the case of image classification, the input $X$ given to the neural network
% is an image, and the output $\hat{y}$ is a probability vector where the largest
% coefficient is the one whose index corresponds to the predicted class of the
% input image. $y_i$, often referred to as the ground truth is a one-hot vector
% where the only non-zero coefficient is at the index corresponding to the true
% class of the input image.\\

% A neural network is a function that maps an input $X$ to an output $y$ through
% a series of transformations. The functional definition of a neural network is
% given in \cref{eqn:dlo:nn_functional_definition}, where $f$ is the neural
% network, $\theta$ is the set of parameters of the network, and $y$ is the
% output.\\


\subsection{Loss Function and Regularisation}
Training a neural network aims at finding the optimal parameters $\theta$ that
maximises a performance, quantified by the metric $P$, often based on the
discrepancy between the predicted output $\hat{y}$ and the true output $y$.
However, optimising directly the metric $P$ might be intractable. To solve for
this issue, we define a differentiable cost function and minimise the latter as
a proxy for optimising $P$. Considering $\delta$ as the empirical distribution
of the training data, the cost function $\mathcal{J}(\theta)$, also denoted the
\emph{empirical risk} is defined as:

\begin{equation}
  \label{eqn:dlo:cost_function}
  \mathcal{J}(\theta) = \mathds{E}_{(X, y) \sim \delta} \left[ \mathcal{L}(f(X,\theta), y) \right]
\end{equation}\\

\noindent where $\mathcal{L}$ is the loss function. Note that the true data
distribution is not known, and thus, the empirical distribution $\delta$ is used
instead. The minimisation of the empirical risk alone is not sufficient. Indeed,
the neural network could learn to perfectly predict the output of the training
set, but fail to generalise to unseen data. This phenomenon is called
\emph{overfitting}. To prevent overfitting, we add a regularisation term to the
empirical risk. The regularisation term, denoted $\mathcal{R}$ is a function of
the parameters $\theta$ of the neural network which penalises the complexity of
the model, and thus prevents overfitting. To account for regularisation, the
cost function in \cref{eqn:dlo:cost_function} is updated to:

\begin{equation}
  \label{eqn:dlo:regularised_cost_fn}
  \mathcal{J}_r(\theta) = \mathds{E}_{(X, y) \sim \delta} \left[ \mathcal{L}(f(X,\theta), y) + \mathcal{R}(\theta) \right]
\end{equation}\\

\noindent\textbf{Loss function.} In
\cref{eqn:dlo:cost_function,eqn:dlo:regularised_cost_fn}, the loss function
$\mathcal{L}$ is a measure of the discrepancy between the ground truth $y$ and
the predicted output. Contrary to the metric $P$ which might be
non-differentiable, the loss function is differentiable so that its minimisation
can be achieved using gradient-based methods, subsequently detailed in
\cref{sec:dlo:backpropagation}. The choice of the loss function depends on the
task at hand. For classification tasks (not only images), the loss function is
often the \emph{cross-entropy} loss. For a binary classification problem, the
ground truth is a binary variable $y\in \{0,1\}$ and the predicted output is a
scalar $f(X,\theta)=\hat{y}\in[0,1]$. The binary cross-entropy loss is defined
as follows:\\

\begin{equation}
  \label{eqn:dlo:binary_cross_entropy_loss}
  \mathcal{L}(\hat{y}, y) = - y \log(\hat{y}) - (1-y) \log(1-\hat{y})
\end{equation}\\

\noindent The binary cross-entropy loss defined in
\cref{eqn:dlo:binary_cross_entropy_loss} can be extended to problems with more
than two classes. For a classification problem with $c$ classes, the ground
truth is a one-hot vector $\mathbf{y}\in \{0,1\}^c$ and the output is a
$c$-dimensional vector $f(X,\theta)=\hat{\mathbf{y}}\in\mathds{R}^c$. The
multi-class cross-entropy loss is defined as follows:

\begin{equation}
  \label{eqn:dlo:multiclass_cross_entropy_loss}
  \mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}) = - \sum_{i=1}^c y_i \log \left( \displaystyle\frac{\exp(\hat{y}_i)}{\displaystyle\sum_{j=1}^c \exp(\hat{y}_j)} \right)
\end{equation}\\

\noindent In the above equation, $\hat{\mathbf{y}}$ is the unormalised raw
output vector of the neural network and $\phi$ is the softmax function, whose
expression is given in \cref{eqn:dlo:softmax}. The softmax function is used to
convert the raw output vector of real numbers into a probability distribution.
Note that some models output directly a probability distribution, in which case
the softmax is not needed.\\

\begin{equation}
  \label{eqn:dlo:softmax}
  \phi(\mathbf{z})_j = \frac{\exp(z_j)}{\displaystyle\sum_{k=1}^{\text{dim}(\mathbf{z})} \exp(z_k)}
\end{equation}\\

\noindent \textbf{Regularisation.} The regularisation term $\mathcal{R}$ is a
differentiable function of the weights $\theta$. It acts as a control mechanism
to avoid overfitting by preventing the weights of the neural network from
becoming too large, which can lead to overly complex models that overfit the
training data. This is typically achieved by adding a penalty proportional to
the magnitude of the weights, thereby keeping them small.\\

Common types of regularisation include $\ell_1$ and $\ell_2$ regularisation,
whose expressions are shown in \cref{eqn:dlo:reg_l1,eqn:dlo:reg_l2}
respectively. $\ell_1$ regularisation \cite{tibshirani1996regression}, adds a
penalty equal to the absolute value of the magnitude of the weights. On the
other hand, $\ell_2$ regularisation \cite{hoerl1970ridge}, adds a penalty
equivalent to the square of the magnitude of the weights. Both methods aim to
reduce the magnitude of the weights, but $\ell_1$ regularisation is more
targeted towards feature selection, effectively pushing some weights to $0$,
whereas $\ell_2$ restrains globally their magnitude.\\

The regularisation term $\mathcal{R}$ is added to the cost function with a
regularisation coefficient, usually denoted as $\lambda$, which is a
hyperparameter that balances the trade-off between fitting the training data
(minimising the loss $\mathcal{L}$) and limiting the complexity of the model
(minimising $\mathcal{R}$).

\begin{equation}
  \label{eqn:dlo:reg_l1}
  \mathcal{R}_{\ell_1}(\theta) = \lambda \sum_{i=1}^{\text{dim}(\theta)} \left| \theta_i \right|
\end{equation}\\

\begin{equation}
  \label{eqn:dlo:reg_l2}
  \mathcal{R}_{\ell_2}(\theta) = \frac{\lambda \| \theta \|_2}{2} 
\end{equation}\\

\subsection{Loss Optimisation}\label{sec:dlo:backpropagation}

% TODO: expliquer le backpropagation avec la chain rule et les gradients, puis
% rapidement stochastique gradient descent and momentum et autres optimiseurs

As mentioned before, the training of a neural network involves finding the
optimal set of parameters $\theta$ that minimises a cost function
$\mathcal{J}(\theta)$. This process of optimisation is typically carried out
using gradient-based methods which rely on the iterative adjustment of the
parameters in the opposite direction of the gradient of the cost function. The
gradient of a function provides the direction of steepest ascent at a given
point \cite{boyd2004convex}. Thus, by moving the parameters in the opposite
direction of the gradient, we seek to descend to a local minimum of the
function.\\

\noindent \textbf{Backpropagation.} One critical step in the optimisation
process is the computation of the gradient of the cost function with respect to
the parameters, $\nabla \mathcal{J}(\theta)$. These gradients are computed with
the \emph{backpropagation} algorithm \cite{rumelhart1986learning} which is an
application of the \emph{chain rule} (see \cref{eqn:dlo:chain_rule}) to
efficiently compute these gradients. It involves a forward pass through the
network to compute the outputs and thus the loss, and a backward pass to
calculate the gradients. During the backward pass, the partial derivative of the
cost with respect to each parameter is computed, starting from the output layer
and going back to the input layer. The previously computed derivatives from the
subsequent layers are used to compute the ones of the earlier layers,  
making the backpropagation algorithm computationally efficient.\\

% Chain rule latex equation
\begin{equation}
  \label{eqn:dlo:chain_rule}
  \frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}
\end{equation}\\

\noindent \textbf{\acl{SGD}.} Once the gradients are calculated, they are
used to update the parameters. The most prevalent for parameter updates is
\acf{SGD}, a derivative of the Robbins–Monro algorithm
\cite{robbins1951stochastic}. In \ac{SGD}, the gradient of the loss function is
computed for a random subset of the data (a \emph{batch} or \emph{mini-batch}),
and the weights are shifted in the direction that decreases the loss function.
This is achieved by subtracting the gradient of the cost function with respect
to that parameter multiplied by a learning rate $\eta$:\\

\begin{equation}
\label{eqn:dlo:sgd_update}
\theta_i^{(t+1)} = \theta_i^{(t)} - \eta \frac{\partial \mathcal{J}(\theta)}{\partial \theta_i}
\end{equation}\\

\noindent where $\theta_i^{(t)}$ is the $i$th parameter at iteration $t$. The
\ac{SGD} algorithm is detailed in \cref{alg:dlo:sgd}. The use of mini-batches in
\ac{SGD} leads to a trade-off between computational efficiency and estimation
accuracy. Indeed, the gradient is estimated using a subset of the entire
training set, which is, on the one hand, less accurate than using the whole
dataset, but on the other hand, less computationally intensive. The size of the
minibatch, which is a hyperparameter of the training algorithm, determines this
trade-off and should also be chosen depending on the computational and memory
resources available. Note that the size of modern datasets, subsequently
detailed in \cref{sec:dlo:datasets}, makes it intractable to evaluate the
gradients on the whole dataset in one step.\\

\begin{algorithm}
  \caption{Stochastic Gradient Descent Algorithm}
  \label{alg:dlo:sgd}
  \begin{algorithmic}
    \REQUIRE {Learning rate $\eta$, minibatch size $m$,Initial parameters
    $\theta^{(0)}$, $n>m$ training pairs $(X,y)\in
    \mathcal{X}\times\mathcal{Y}$, Loss function $\mathcal{J}$}
    \WHILE {Stopping criterion not met}
    \STATE {Sample minibatch of size $m$ from training set}
    \STATE {Compute gradient estimate on minibatch: $\hat{g} \gets \nabla \mathcal{J}(\theta)$}
    \STATE {Update parameters: $\theta^{(t+1)} \gets \theta^{(t)} - \eta \hat{g}$}
    \ENDWHILE
    \RETURN {Optimal parameters $\theta$}
  \end{algorithmic}
\end{algorithm}


\noindent \textbf{Learning Rate.} The learning rate is a hyperparameter that
determines the step size during each iteration while moving toward a minimum of
a loss function. Setting the learning rate too high can cause the learning
process to converge too quickly or overshoot while setting it too low can make
the learning process slow to converge, as shown in
\cref{fig:dlo:gradient_descent}.\\

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{chapter_dlo/assets/gradient_descent.pdf}
  \caption{Illustration of the effect of the learning rate on the convergence of
    the gradient descent. The gradient descent have beed applied iteratively for
    20 epochs. On the one hand, a too-high learning (1.01) rate causes the
    gradient descent to overshoot the minimum of the loss function. On the other
    hand, a too-low learning rate (0.01) causes the gradient descent to converge
    slowly.}
  \label{fig:dlo:gradient_descent}
\end{figure}

\noindent \textbf{Alternative methods.} To enhance the performance of \ac{SGD},
various modifications and extensions have been proposed, such as \ac{SGD} with
momentum \cite{sutskever2013importance}, RMSProp \cite{hinton2012neural}, or
Adam \cite{kingma2014adam}. These methods aim to adjust the learning rate
dynamically or dampen the oscillations in the gradient descent to achieve faster
and more stable convergence.\\

For instance, \ac{SGD} with momentum \cite{sutskever2013importance} uses a
momentum term $\gamma$ to dampen the oscillations in the gradient descent. It
allows smoothing the variations of the descent direction and thus, preventing
the optimisation to get stuck in small local minimums. The momentum term is a
moving average of the gradients, here denoted $v$, and it is used to update the
parameters as shown in \cref{eqn:dlo:sgd_momentum_update}. In this equation, the
momentum term $\gamma \in [0,1]$ is a hyperparameter that is typically set close
to 1, $0.9$ being a common value.\\

\begin{equation}
  \label{eqn:dlo:sgd_momentum_update}
  \begin{split}
    v_{t+1} &= \gamma v_t + \eta \nabla \mathcal{J}(\theta) \\
    \theta_{t+1} &= \theta_t - v_{t+1}
  \end{split}
\end{equation}\\

\section{Convolutional Neural Networks for Computer Vision}\label{sec:dlo:cnn}

%TODO: intro de cette partie

\subsection{Building Blocks}

%TODO: intro de cette partie

\noindent\textbf{Convolutional layer.} Convolutional layers are one of the core
building blocks of \acp{CNN}. Each convolution layer performs a series of
spatial convolutions on the input data using a set of learnable filters or
kernels. These filters are designed to extract low-level features such as edges,
corners, and textures in the early layers, while they learn high-level features
like object parts or even whole objects in the deeper layers. In contrary to
manual feature engineering, the features learned by convolutional layers are
learned in a \emph{end-to-end} fashion. The convolution operation is defined in
\cref{eqn:dlo:convolution} :\\

\begin{equation}
  \label{eqn:dlo:convolution}
  (x * k)(i,j) = \sum_{m} \sum_{n} x(i -m, j -n) \cdot k(m, n)
\end{equation}\\

\noindent where, $x$ is the input, $k$ is the kernel, $(i,j)$ are the spatial
coordinates in the output feature map and $m$ and $n$ represents the spatial
coordinates of the kernel $k$. Note that some Deep Learning frameworks
implements \emph{cross-correlation} instead of convolution. In the former, the
kernel is not spatially flipped leading to the cross-corelation not being
commutative \cite{goodfellow2016deep}. The convolutional layer kernels are
typically smaller than the input along width and height dimensions (they are
generaly $3\times 3$ \cite{DBLP:conf/cvpr/HeZRS16}) but comprises as much
channels as the input. During the forward pass, each kernel is spatially
convolved channel-wise with the input and their outputs are summed along the
channel dimension to yield a single scalar for each kernel position on the
input.\\


\noindent \textbf{Activation function.} Activation function are often applied to
the output feature map of a convolutional or fully connected layer. This
function introduces non-linearity into the model, allowing it to learn more
complex patterns \cite{long2015fully}. A common activation function used in
\acp{CNN} is the \ac{ReLU} function, represented as $f(x)=\max(0,x)$, although
other functions like the sigmoid $f(x)=1/(1+e^{-x})$ or tanh $f(x)=(e^{x}
-e^{-x})/(e^{x}+e^{-x})$ functions have been used. The \ac{ReLU} is prefered
over the latter for its computational efficiency and its ability to mitigate the
vanishing or exploding gradients problem
\cite{hochreiter2001gradient,glorot2010understanding}.\\

\noindent \textbf{Pooling.} Pooling is often employed after convolutional layers
in a \ac{CNN} and aims at progressively reducing the spatial size of the input
representation, thus reducing the number of parameters and computations in the
network. This also helps control overfitting, and increase the receptive field
of the subsequent layers. The pooling operation is performed independently on
each input channel, so the depth dimension remains unchanged. The two most
common types of pooling are \emph{max pooling} and \emph{average pooling}. with
the former, the maximum value in each window (often of size $2\times 2$) is
selected, while the latter computes the average value of the window. Given an
input matrix $\mathbf{x}$ and a pooling function $p$, the output matrix $
\mathbf{y} $ for a certain spatial location $(i, j)$ is defined as follows:\\

\begin{equation}
  \label{eqn:dlo:pooling}
  y_{ij} = p \left( \mathbf{x}(i⋅s:i⋅s+f,j⋅s:j⋅s+f) \right)
\end{equation}\\

\noindent where $s$ is the stride or step size, $f$ is the spatial extent of the
filter, or window size and $\mathbf{x}(m:n)$ denotes a slice of the vector or
matrix $\mathbf{x}$ starting at index $m$ and ending at index $n$. Note that
pooling have no learnable parameters. They only downsample the input based on a
fixed function.\\


\noindent \textbf{Batch Normalisation.} \ac{BN} is a technique introduced in
\cite{DBLP:conf/icml/IoffeS15} to combat the issue of internal covariate shift
in deep neural networks, thereby accelerating training and improving
generalization. Covariate shift refers to the change in the input distribution
to a learning system, which can lead to slow convergence and make the network
harder to train. \ac{BN} normalises the input layer by adjusting and scaling the
activations. For each mini-batch, it computes the mean and variance of the
activations and performs normalization. The transformation is defined as
follows:\\

\begin{equation}
  \label{eqn:dlo:batchnorm}
  \hat{x}_{i} = \frac{x_{i} - \mu_{B}}{\sqrt{\sigma_{B}^{2} + \varepsilon}}
\end{equation}\\

\noindent where $x_{i}$ is the input, $\mu_{B}$ is the mini-batch mean,
$\sigma_{B}^{2}$ is the mini-batch variance, and $\varepsilon$ is a small
constant for numerical stability. After normalization, the method allows the
network to learn an affine transformation for each activation, permitting the
network to control the mean and standard deviation of the input distribution,
formalised in \cref{eqn:dlo:batchnorm}.\\

\begin{equation}
  \label{eqn:dlo:batchnorm_affine}
  y_{i} = \gamma \hat{x}_{i} + \beta
\end{equation}\\

\noindent where, $\gamma$ and $\beta$ are the learnable parameters of the affine
transformation. \ac{BN} has the advantage of making the network less sensitive
to the initial weights, allowing higher learning rates, and reducing the need
for Dropout, among other regularisers. However, its effectiveness decreases in
the case of small batch sizes, as the estimate of the batch mean and variance
becomes less accurate.\\

\noindent \textbf{Fully connected layer.} \ac{FC} layers, also known as
\emph{Dense} layers are often the last layers of a \ac{CNN}, effectively serving
as a classifier, whereas the convolutional layers act as a feature extractor.
\ac{FC} layers perform high-level reasoning by conducting non-linear
transformations of the extracted features and combining them to make decisions.
In an FC layer, each neuron is connected to every neuron in the previous layer.
A \ac{FC} layer can be describe as a matrix-vector product as in
\cref{eqn:dlo:fc_layer}.\\

\begin{equation}
  \label{eqn:dlo:fc_layer}
  y = \mathbf{w}^T \cdot \mathbf{x} + b
\end{equation}

\noindent where $\mathbf{x}$ is the input vector, $\mathbf{w}$ is the weight
matrix and $b$ is the bias. In the context of \acp{CNN}, before passing the
output of the last convolutional layer to the first \ac{FC} layer, it needs to
be flattened or reshaped into a single column vector. The final layer in a
\ac{CNN} is a \ac{FC} layer that has a number of neurons equal to the number of
output classes, and it typically uses a softmax activation to output a
probability distribution over those classes.\\

%TODO: rajouter phrase pour dire que l'on peut remplacer les layers FC par des
%layers conv pour des réseaux "fully convolutional"

%TODO: traiter le paragraphe Dropout.

\noindent \textbf{Dropout.} Dropout is a regularization technique used to
prevent overfitting in neural networks, including Convolutional Neural Networks
(CNNs). Overfitting occurs when a model learns to perform well on the training
data but does not generalise well to unseen data. By applying Dropout, we aim to
enhance the model's ability to generalise, thereby improving its performance on
validation and test data.\\

Dropout, introduced by Hinton et al., works by randomly deactivating a
proportion of neurons in a layer during each training iteration [1]. More
specifically, during the forward pass, each neuron has a probability \$p\$ of
being temporarily removed from the network, effectively breaking up
co-adaptations between neurons and forcing them to learn more robust and
independent features.\\

Mathematically, the operation can be described as follows. Let \$x\$ denote the
activations of a certain layer, and \$r\$ be a binary mask vector of the same
shape as \$x\$, where each element of \$r\$ is independently drawn from a
Bernoulli distribution with success probability \$p\$:\\

\begin{equation}
  \label{eqn:dlo:dropout}
  r \sim \text{Bernoulli}(p)
\end{equation}

∼Bernoulli(p) The output \$y\$ after applying Dropout can then be expressed as:\\

\begin{equation}
  \label{eqn:dlo:dropout_output}
  y = \frac{x \odot r}{p}
\end{equation}

where \$*\$ denotes element-wise multiplication.\\

The network with Dropout can be seen as a collection of possible smaller
sub-networks, each trained on a different subset of the data. During testing,
all neurons are used but their outputs are scaled down by factor \$p\$ to
account for the larger number of active units at test time, leading to a sort of
``averaging'' effect over the learned sub-networks [1].\\

It's important to note that although Dropout has proven to be an effective tool
in preventing overfitting, it may not always lead to the best performance,
especially when used with modern normalization techniques like Batch
Normalization [2].\\

\subsection{Architectures}\label{sec:dlo:architectures}

\section{Datasets}\label{sec:dlo:datasets}

In this thesis manuscript, we focuses on image classification tasks and
supervised learning. Supervised learning is a machine learning paradigm in which
the model is trained using labelled data. In the context of image
classification, the input data is an image, and the label is the class of the
image. We denote an input image $X$ and its corresponding label $y$. Each image
$X$ belongs to the set of all images of the dataset $\mathcal{X}$, and each
label $y$ belongs to the set of all labels of the dataset $\mathcal{Y}$. The
dataset, denoted $\mathcal{D}$, is a set of pairs $(X, y)$, where $X \in
  \mathcal{X}$ and $y \in \mathcal{Y}$, so that $D \subset \mathcal{X} \times
  \mathcal{Y}$. \\

In our experiments, we evaluated our methods on three different datasets
tailored for image classification: CIFAR-10 \cite{CIFARdataset}, CIFAR-100
\cite{CIFARdataset} and TinyImageNet \cite{TinyImageNet}. The following
paragraphs give details about these datasets and \cref{tab:dlo:datasets} sums
up their main characteristics.\\

\begin{table}[ht!]
  \centering
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Dataset}    & \textbf{Number of images} & \textbf{Number of classes} &
    \textbf{Image size} & \textbf{Size of test set}                                               \\
    \hline
    CIFAR-10            & 60,000                    & 10                         & 32x32 & 10,000 \\
    CIFAR-100           & 60,000                    & 100                        & 32x32 & 10,000 \\
    TinyImageNet        & 100,000                   & 200                        & 64x64 & 10,000 \\
    \bottomrule
  \end{tabular}
  \caption{The number of images, of classes, image size and size of the test
    set for the three datasets used: CIFAR-10, CIFAR-100 and TinyImageNet.}
  \label{tab:dlo:datasets}
\end{table}

\subsection{CIFAR-10}

The CIFAR-10 dataset \cite{CIFARdataset} is a widely
used dataset in machine learning and computer vision. This is a labeled subset
of the \emph{80 Millions Tiny Images} dataset \cite{4531741}. CIFAR-10 is a
simple yet challenging dataset that allows for quicker iteration or
hyperparameter tuning than larger datasets such as ImageNet
\cite{DBLP:journals/ijcv/RussakovskyDSKS15}, but it is significanly more complex
than the MNIST dataset \cite{6296535}, which contains grayscale handwritten
digits images. the CIFAR-10 dataset contains 60,000 colour images of size 32x32
pixels, split into 10 classes, namely: plane, car, bird, cat, deer, dog, horse,
ship, truck. Each class contains 6,000 images. The dataset is divided into two
sets: a training set, composed of 50,000 images and a test set containing 10,000
of them.\\

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7\textwidth]{chapter_dlo/assets/cifar-10_example.png}
  \caption{ A sample grid of images from the CIFAR-10 dataset. Each row
    contains images from one of the 10 classes: plane, car, bird, cat,
    deer, dog, frog, horse, ship, and truck}
  \label{fig:intro:cifar10_examples}
\end{figure}


\subsection{CIFAR-100}

CIFAR-100 \cite{CIFARdataset} is a more challenging
version of CIFAR-10. Like the latter, it is a labeled subset of the \emph{80
  Millions Tiny Images} and  is composed of 60,000 colour images of size 32x32
pixels. However, instead of 10 classes, CIFAR-100 contains 100 classes of 600
images each. As a result, each class has far fewer images than in CIFAR-10.
CIFAR-100 is also divided into two sets: a training set and a test, composed of
50,000 and 10,000 images respectively.\\

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7\textwidth]{chapter_dlo/assets/cifar-100_example.png}
  \caption{A sample grid of images from the CIFAR-100 dataset. The images
    represent a subset of the 100 available classes, each image represents
    a unique class.}
  \label{fig:intro:cifar100_examples}
\end{figure}


\subsection{TinyImageNet}

TinyImageNet dataset is another popular dataset
in machine learning and computer vision, conceived as a subset of the larger
ImageNet dataset \cite{DBLP:journals/ijcv/RussakovskyDSKS15}. It comprises
100,000 colour images of size 64x64 pixels, split into 200 classes, whereas
ImageNet contains 1.2 million images of size 256x256 pixels, split into 1,000
classes. The dataset is divided in 3 sets: the train set, containing 500
images per class, the validation and test sets, both containing 50. The scaled
down image size and count make TinyImageNet more computationally accessible than
ImageNet while still being a challenging task and maintening the diversity of
the images and classes.\\


\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7\textwidth]{chapter_dlo/assets/tinyimagenet_example.png}
  \caption{A sample grid of images from the Tiny ImageNet dataset. The images
    represent a subset of the 200 distinct classes, each image represents a
    unique class.}
  \label{fig:intro:tinyimagenet_examples}
\end{figure}

% In addition to these two folds, we
% also use a validation set to tune the hyperparameters of our method before
% applying it on the test set. The validation dataset is obtained by selecting
% 10\% of the training set. \Cref{tab:dlo:datasets} summaries the composition of
% the three datasets.\\ 

% The CIFAR-10 and CIFAR-100 datasets both contain 60 000 colour images of size
% 32x32 pixels, split into 10 and 100 classes, respectively. The TinyImageNet
% dataset is a shrunk version of the ImageNet dataset
% \cite{DBLP:journals/ijcv/RussakovskyDSKS15}, and contains 100 000 images of size
% 64x64 pixels, split into 200 classes. Each dataset is divided into two folds: a
% training set and a test set. In addition to these two folds, we also use a
% validation set to tune the hyperparameters of our method before applying it on
% the test set. The validation dataset is obtained by selecting 10\% of the
% training set. \Cref{tab:dlo:datasets} summaries the composition of the three
% datasets.\\

% In combination with these datasets, we use four different neural networks. Conv4
% is a reasonably small convolutional neural network which is a
% shrunk-down version of the VGG16 architecture. VGG16 is a \acl{CNN}
% of larger size and depth, which is a popular choice for image classification. We
% used a slightly modified version of VGG16 that is better suited to CIFAR-10. We
% do not use dropout, but we use batch normalisation, and the fully connected
% section has only one layer. ResNet18 is a residual neural network that
% introduces skip connections in its architectural design. ResNet20 is a modified
% version of the ResNet18 architecture to make it suited for CIFAR-10 and CIFAR-100
% datasets. \Cref{tab:intro:networks_size} summarises the size of the different
% models. In our experiments, we use ResNet18 exclusively for TinyImageNet and the
% other networks for CIFAR-10 and CIFAR-100.\\

\subsection{Train, Validation and Test Sets}

In our experiments, for each dataset, we use 3 sets: train, validation and test
sets. The training set serves to train the model, while the validation set is
used to monitor the evolution of performance metrics on unseen data throughout
the training. Validation metrics provides the necessary triggers for the early
stopping policy (\emph{i.e.} interrupting the training prematurely if the
validation metrics do not change over a given number of iterations). The test
set, on the other hand, is used to evaluate the model's performance on entirely
new data and reporting the final test accuracy. When utilizing datasets like
CIFAR-10 and CIFAR-100, only training and testing sets are available. For these
datasets, we split the given train set in half with the following proportions:
90\% of the original training set is used for training for the network and the
remaining 10\% is used as the validation set. On the other hand, the
TinyImageNet dataset does provide training, validation, and testing sets, but
the test set lacks annotations. Hence, we use 90\% of the training set for model
training and the remaining 10\% for validation. Instead of the unannotated test
set, we repurpose the original validation set to serve as the test set. This is
a common strategy employed by other implementations
\cite{hanyuanxu2018tinyimagenet,nbdt,alvinwan2020nbdt}.\\

% \section{Architectures}\label{sec:intro:architectures}

% Tiré du chapitre 2
Conv2 and Conv6 are modified versions of the Conv4 architecture, featuring 2 and
6 convolutional layers, respectively, instead of the original 4 convolutional
layers. The other networks mentioned are described in
\cref{sec:chap1:experiments}. The number of parameters for these architectures
is provided in \cref{tab:chap2:conv_num_params}. Although the Conv2, Conv4 and
Conv6 networks introduced by \citeauthor{DBLP:conf/iclr/FrankleC19} in
\cite{DBLP:conf/iclr/FrankleC19} are not widely featured in existing literature,
we chose to employ them due to their use in the methods we benchmark against.

\begin{table}[ht!]
  \centering\begin{tabular}{lccc}
    \cmidrule[\heavyrulewidth]{2-4}
                         & \textbf{Conv2} & \textbf{Conv4} & \textbf{Conv6} \\ \toprule
    Number of Parameters & 4,301,642      & 2,425,930      & 2,262,602      \\ \bottomrule
  \end{tabular}
  \caption{Number of parameters for the Conv2, Conv4 and Conv6 architectures, when used with the CIFAR-10 dataset}
  \label{tab:chap2:conv_num_params}
\end{table}

\begin{table}[ht!]
  \centering
  \begin{tabular}{lcccc}
    \cline{2-5}
                         & \textbf{Conv4} & \textbf{VGG16} & \textbf{ResNet20} & \textbf{ResNet18} \\ \hline
    Number of Parameters & 2,425,930      & 14,728,266     & 269,034           & 11,685,608        \\ \hline
  \end{tabular}
  \caption{ number of parameters for the four used neural network architectures.}
  \label{tab:intro:networks_size}
\end{table}