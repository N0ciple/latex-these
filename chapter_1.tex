\chapter{Weight Reparametrization}

\begin{abstract}
    abstract of this part
\end{abstract}

\section{Introduction And Related Work}

The introduction of neural networks has revolutionised the field of machine
learning, leading to breakthroughs in various areas such as image and speech
recognition, natural language processing, and game playing. However, the size of
neural networks has steadily increased in recent years, largely thanks to the
availability of powerful \ac{GPUs}. They have made it possible to train larger
and more complex models. However, as the size of neural networks has grown, so
have the computational and memory requirements to train and deploy them. The
evolution of neural network architectures can be traced back to the Rosenblatt
Perceptron \cite{rosenblatt1958perceptron}, a single-layer feedforward network
with only one neuron. Over time, neural network architectures became more
complex and began to include multiple layers, known as multilayer perceptron, or
fully connected networks. Then, with the introduction of \ac{CNNs}, neural
network architectures for image recognition have grown even larger.
Convolutional neural networks use convolutional layers to automatically and
adaptively learn spatial hierarchies of features from input images. This allows
\ac{CNNs} to effectively learn and classify images with high accuracy. Some of
the most notable \ac{CNN} architectures include AlexNet
\cite{DBLP:conf/nips/KrizhevskySH12}, which was developed in 2012 and has 60
million parameters. The VGG networks \cite{DBLP:journals/corr/SimonyanZ14a},
developed in 2014, are ranging from 132 to 143 million parameters. Inception
\cite{DBLP:conf/cvpr/SzegedyLJSRAEVR15}, developed in 2014, had 27 million
parameters in its third version. And the ResNet networks
\cite{DBLP:conf/cvpr/HeZRS16}, developed in 2015, are ranging from 11 to 60
million parameters. \\


The need for neural networks in embedded applications has grown in recent years,
with examples such as object detection in self-driving cars, image and speech
recognition in mobile devices, and natural language processing in smart
speakers. These applications require real-time processing and low power
consumption, which are impossible with large neural networks. Pruning techniques
can reduce the size of neural networks, making them more suitable for deployment
on embedded devices while still maintaining or even improving their performance.
Methods such as structured and unstructured weight pruning can reduce the number
of parameters and FLOPS, consequently reducing the network's size, memory and
power consumption.\\


Pruning is an excellent way to obtain lightweight neural networks because it
reduces the number of parameters in a pre-trained network without the need to
design a new architecture from the ground up. Instead of starting from scratch,
pruning techniques can be applied to existing architectures, which have been
trained and tested on large-scale datasets. It aims at reducing the number of
parameters in a network by removing redundant or unnecessary weights. Pruning
methods can be split into two major categories: unstructured weight pruning,
where individual weights of the network are removed based on their importance.
And structured pruning, where entire columns, rows, channels, filters or even
subnetworks are removed. \\


The first methods to prune shallow networks were proposed in the late 1980s.
Techniques from that area include removing the smallest connection
\cite{janowsky1989pruning}, introducing a weight scaling factor and the study of
its impact on the loss function \cite{DBLP:conf/nips/MozerS88} or the study of
the sensitivity of the weights based on the gradients
\cite{DBLP:journals/tnn/Karnin90}. Most influential papers of the early pruning
days are Optimal Brain Damage \cite{DBLP:conf/nips/CunDS89} and Optimal Brain
Surgeon
\cite{DBLP:conf/nips/HassibiS92,DBLP:conf/nips/HassibiSW93,DBLP:conf/icnn/HassibiSW93}.
The former work focuses on pruning weights based on their impact on the loss,
approximated by its Taylor series, which requires the computation of the hessian
matrix of the loss. However, computing the hessian matrix is Intractable in
practice due to the large number of parameters in the neural networks.
Therefore, the authors introduced a few simplifying assumptions, most notably
the diagonal assumption for the hessian matrix: loss perturbations following
weight pruning are assumed to be weight independent. \\


More recently, pruning regained traction with the work of Han et al.
\cite{DBLP:conf/nips/HanPTD15}.  The authors introduced a three-step pruning
method where first, the weights are tuned. Then all the weights whose absolute
value is below a certain threshold are removed. Finally, the remaining weights
are finetuned. \\


Following this work, research efforts stirred toward structured pruning.
Structured pruning removes groups of weights. The substructure of this group can
be a simple row or column in a filter, a channel of a filter, the filter itself
or even entire subnetworks. Structured pruning is not sparsifying weight
tensors, but rather reshaping the network to remove unnecessary parts of it that
are costly to evaluate and do not bring much performance improvement regarding
the considered task. Since the remaining weight tensors are not sparse, speedups
can be achieved with conventional libraries and hardware. In this context, Anwar
et al. \cite{anwar2017structured} proposed a pruning technique on various levels
(channels, kernels and intra-kernel levels) while Li et al.
\cite{DBLP:conf/iclr/0022KDSG17} proposed a pruning method at a larger (filter)
level. Network slimming \cite{DBLP:conf/iccv/LiuLSHYZ17} is a streamlined
approach that aims at pruning the most useless channels in the layers preceding
\ac{batch norm} \cite{DBLP:conf/icml/IoffeS15}. It induces sparsity with
$\ell_1$ penalization of the \ac{batch norm} scaling factors, each one
associated with a channel. Then channels are removed based on the relative
importance of their associated scaling factor, up to a predefined sparsity
ratio. More recent work proposed an automatic policy for pruning, such as
\ac{amc} \cite{DBLP:conf/eccv/HeLLWLH18} which relies on reinforcement learning
with two interacting agents; the first one iterates over the layers of the
architecture and defines a targeted sparsity, and the second agent implements
the targeted sparsity using channel pruning. The \ac{amc} algorithm is either
constrained by accuracy or efficiency, depending on the reward assigned to the
agents.  Going further with the concept of automatic pruning and architecture
search, Ramakrishnan et al. \cite{DBLP:conf/crv/RamakrishnanSN20} adapt $\ell_1$
penalization from \cite{DBLP:conf/iccv/LiuLSHYZ17} to model the relative
importance of layers, groups of layers or network parts that can be removed.
Following the same line, \cite{DBLP:conf/icml/KangH20} proposed a channel
pruning method based on batch normalisation parameters. The authors introduce
masks which model the likelihood of feature maps being inhibited by the ReLU
activation function and thereby not contributing to the evaluation of the
underlying network. These masks are obtained by binarizing the cumulative
density function of the gaussian distribution parameterised by the scaling and
the shift of the BN layer. Masks and BN parameters are updated “end-to-end” with
a gradient estimated using the Gumble Softmax trick
\cite{DBLP:conf/iclr/JangGP17}. Authors claim that a high accuracy is maintained
after pruning and without fine-tuning. \\



Although convenient to implement in practice, structured pruning imposes a
strong topological prior by removing whole chunks in the primary network and
achieves a lower sparsity rate compared to unstructured pruning. On the other
hand, unstructured weight pruning focuses on removing independent weights from
the global structure. As a result, this method is much more flexible and leads
to high sparsity rates and compression ratios. Han et al.
\cite{DBLP:conf/nips/HanPTD15} introduced a simple yet effective three-step
algorithm for unstructured weight pruning: a first standard training step to
identify the most important connections, a magnitude pruning step to remove the
smallest weight and a final finetuning step to compensate for the loss of
accuracy. \cite{DBLP:journals/corr/HanMD15} used the same technique in
combination with quantisation and Huffman coding, achieving a compression ratio
of up to 49x for a VGG16 network. Other methods do not rely on weight magnitude
such as \cite{DBLP:conf/iclr/LouizosWK18}, which uses non-negative stochastic
gates as a surrogate L0 norm and penalise non-zero weights during training.
Variational Dropout \cite{DBLP:conf/icml/MolchanovAV17} introduces a
multiplicative gaussian noise as an alternative to binary dropout
\cite{DBLP:journals/corr/abs-1207-0580,DBLP:journals/jmlr/SrivastavaHKSS14} with
an unbound dropout rate. Magnitude pruning regains significant attention after
the publication of the Lottery Ticket Hypothesis
\cite{DBLP:conf/iclr/FrankleC19}ttery Tickets, whose training with initial
weights taken from the large networks yields comparably accurate classifiers. To
extract the lottery ticket, it is necessary to train the large network up to
convergence, apply magnitude pruning and restore the original values of the
unpruned weights. This Lottery Ticket can then be trained to match the level of
performances of the large network, with at most the same number of epochs
needed. Although remarkable, this result is hardly applicable in practice since
it requires multiple computationally intensive training steps.\\


These structured or unstructured methods propose different saliency indicators
and pruning criteria that aim at identifying and removing redundant or
unnecessary weights or groups of weights in order to remove them. Removing
weights introduces a loss of functional performance - depending on the task
considered - that needs to be compensated for (with the exception of
\cite{DBLP:conf/icml/KangH20}). This is achieved through finetuning the sparse
or lightened networks obtained after applying the pruning criterion. Finetuning
is a computationally intensive task and requires additional training time.
Moreover, the amount of weights pruned is enforced after the initial training,
meaning that the final target size or weight budget is never considered in the
optimisation procedure. Hence the need for a finetuning step. \\


In order to address the aforementioned issues, we introduce a novel
reparametrisation that learns not only the weights of a surrogate lightweight
network but also its topology. This reparametrisation acts as a regulariser that
models the tensor of the parameters of the surrogate network as the Hadamard
product of a weight tensor and an implicit mask. The latter makes it possible to
implement unstructured pruning constrained with a budget loss that precisely
controls the number of nonzero connections in the resulting network. Experiments
conducted on the CIFAR10 and the TinyImageNet classification tasks, using
standard primary architectures (namely Conv4, VGG19 and ResNet18), show the
ability of our method to train effective surrogate pruned networks without any
fine-tuning.

\section{Pruning With Weight Reparametrisation And Budget Loss}

Consider, in the general case, a multi-layer neural network as a function $f$ of
two variables: $\theta$ and $X$. $\theta$ is the set of parameters of the
network, so that $\theta = \{W_1, W_2, \ldots, W_L\}$, where $L$ is the number
of layer of the network,  and $X$ is the input. The input $X$ is an element of a
dataset $\mathcal{D}=\{ \mathcal{X}, \mathcal{Y} \}$, where $\mathcal{X}$ is the
set of the input data, and $\mathcal{Y}$ is the set of the corresponding labels.
Evaluating the neural network $f(\theta, X_i)$ yields the output $\hat{y_i}$
which is the prediction of the network for the input $X_i$. The discrepancy
between the ouput of the neural network $\hat{y_i}$ and the ground truth $y_i
\in \mathcal{Y}$ is computed with a loss function $\mathcal{L}$.  This loss is
then minimized by updating the parameters $\theta$ of the network, thanks to the
backpropagation \cite{rumelhart1985learning,rumelhart1986learning} and gradient
descent methods. \\

The $L_0$ norm is perferctly suited for introducing sparsity in a network by, on
one hand, acting as a sparsity inducing regulariser for the weights, and on the
other hand, by indicating the number of non-zero weights in the network, which
is useful for computing the weight budget. \\

Our aim is to propose an end-to-end method that fits into the backpropagation
framework. Therefore, adding a $L_0$ regulariser and a $L_0$ based weight budget
is not possible, since $L_0$ norm is not differentiable. Thus we propose our
differentiable reparametrization, which seeks to define a novel weight expression
related to magnitude pruning
\cite{DBLP:conf/nips/CunDS89,DBLP:conf/nips/HanPTD15}. This expression
corresponds to the Hadamard product involving a weight tensor and a function
applied entry-wise to the same tensor (as shown in fig. 1). This function acts
as a mask that i) multiplies weights by soft-pruning factors which capture their
importance and ii) pushes less important weights to zero through a particular
budget added to the loss function $\mathcal{L}$. \\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{}
    \caption{Weight reparametrisation}
    \label{fig:reparametrisation}
\end{figure}

Our proposed framework allows for a joint optimization of the network weights
and topology. On the one hand, it prevents disconnections which may lead to
degenerate networks with irrecoverable performance drop. On the other hand, it
allows reaching a targeted pruning budget in a more convenient way than $L_1$
regularization. Our reparametrization also helps minimizing the discrepancy
between the primary and the surrogate networks by maintaining competitive
performances without fine-tuning. Learning the surrogate network requires only
one step that achieves pruning as a part of network design. This step zeroes out
the targeted number of connections by constraining their reparametrized weights
to vanish.

\subsection{Weight Reparametrisation}

\subsection{Budget Loss}