\chapter{Introduction}
\section{}
\subsection{Datasets}

\textbf{CIFAR-10}


The CIFAR-10 and CIFAR-100 datasets both contain 60 000 colour images of size
32x32 pixels, split into 10 and 100 classes, respectively. The TinyImageNet
dataset is a shrunk version of the ImageNet dataset
\cite{DBLP:journals/ijcv/RussakovskyDSKS15}, and contains 100 000 images of size
64x64 pixels, split into 200 classes. Each dataset is divided into two folds: a
training set and a test set. In addition to these two folds, we also use a
validation set to tune the hyperparameters of our method before applying it on
the test set. The validation dataset is obtained by selecting 10\% of the
training set. \Cref{tab:chap1:datasets} summaries the composition of the three
datasets.\\


\begin{table}[ht]
  \centering
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Dataset}    & \textbf{Number of images} & \textbf{Number of classes} &
    \textbf{Image size} & \textbf{Size of test set}                                               \\
    \hline
    CIFAR-10             & 60,000                    & 10                         & 32x32 & 10,000 \\
    CIFAR-100            & 60,000                    & 100                        & 32x32 & 10,000 \\
    TinyImageNet        & 100,000                   & 200                        & 64x64 & 10,000 \\
    \bottomrule
  \end{tabular}
  \caption{The number of images, of classes, image size and size of the test set for the three datasets used.}
  \label{tab:chap1:datasets}
\end{table}

In combination with these datasets, we use four different neural networks. Conv4
is a reasonably small convolutional neural network which is a
shrunk-down version of the VGG16 architecture. VGG16 is a \acl{CNN}
of larger size and depth, which is a popular choice for image classification. We
used a slightly modified version of VGG16 that is better suited to CIFAR-10. We
do not use dropout, but we use batch normalisation, and the fully connected
section has only one layer. ResNet18 is a residual neural network that
introduces skip connections in its architectural design. ResNet20 is a modified
version of the ResNet18 architecture to make it suited for CIFAR-10 and CIFAR-100
datasets. \Cref{tab:intro:networks_size} summarises the size of the different
models. In our experiments, we use ResNet18 exclusively for TinyImageNet and the
other networks for CIFAR-10 and CIFAR-100.\\


\begin{table}[ht]
  \centering
  \begin{tabular}{lcccc}
    \cline{2-5}
                         & \textbf{Conv4} & \textbf{VGG16} & \textbf{ResNet20} & \textbf{ResNet18} \\ \hline
    Number of Parameters & 2,425,930      & 14,728,266     & 269,034           & 11,685,608        \\ \hline
  \end{tabular}
  \caption{ number of parameters for the four used neural network architectures.}
  \label{tab:intro:networks_size}
\end{table}

\subsection{Networks}

% Tir√© du chapitre 2
Conv2 and Conv6 are modified versions of the Conv4 architecture, featuring 2 and
6 convolutional layers, respectively, instead of the original 4 convolutional
layers. The other networks mentioned are described in
\cref{sec:chap1:experiments}. The number of parameters for these architectures
is provided in \cref{tab:chap2:conv_num_params}. Although the Conv2, Conv4 and
Conv6 networks introduced by \citeauthor{DBLP:conf/iclr/FrankleC19} in
\cite{DBLP:conf/iclr/FrankleC19} are not widely featured in existing literature,
we chose to employ them due to their use in the methods we benchmark against.

\begin{table}[htbp]
    \centering\begin{tabular}{lccc}
      \cmidrule[\heavyrulewidth]{2-4}
                           & \textbf{Conv2} & \textbf{Conv4} & \textbf{Conv6} \\ \toprule
      Number of Parameters & 4,301,642      & 2,425,930      & 2,262,602      \\ \bottomrule
    \end{tabular}
    \caption{Number of parameters for the Conv2, Conv4 and Conv6 architectures, when used with the CIFAR-10 dataset}
    \label{tab:chap2:conv_num_params}
  \end{table}