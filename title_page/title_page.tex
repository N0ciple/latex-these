\title{\vspace{-3.0cm}Manuscript}
\author{Robin Dupont}
\date{}

\newpage

% abstract
\begin{abstract}

    Since the 2010s, thanks to the miniaturisation of electronics,
    embedded devices have become more and more ubiquitous, realising various
    tasks all around us. As their usage is developing, there is a growing demand
    for these devices to process data and make complex decisions quickly and
    efficiently. Deep neural networks are a powerful tool to achieve this goal,
    however, these networks are often too heavy and complex to fit on embedded
    devices. Thus, there is a compelling need to devise methods to compress
    these large networks without significantly compromising their efficacy. this
    thesis introduces two innovative methods, centred around the concept of
    pruning, aiming to compress neural networks while ensuring minimal impact on
    their efficacy.

    In this thesis, we first present a budget-aware method for compressing large
    neural networks with weight reparametrization and budget loss that does not
    requires fine-tuning. Traditional pruning methods often rely on
    post-training saliency indicators to remove weights, disregarding the
    targeted pruning rate. Our approach integrates a budget loss, driving the
    pruning process towards a specific rate during training itself, therefore
    allowing for joint optimisation of topology and weights. By soft-pruning the
    smallest weights using our weight reparametrisation technique, our method
    significantly mitigates performance degradation in comparison to traditional
    pruning techniques. We show the effectiveness of our approach across various
    datasets, architectures, and settings.

    This thesis later focuses on the extraction of effective subnetworks without
    weight training. The goal here is to only identify the best sub-topology in
    a large network with no optimisation of the weights that still delivers
    compelling performance. We introduce the \acl{ASLP} that enables discrete
    yet differentiable topology sampling. Alongside, the introduced \acl{SR}
    mechanism rescales weight distributions, enhancing the performance of the
    extracted subnetwork and speeding up their training convergence. Our pruning
    strategy further identifies optimal pruning rates post-training,
    side-stepping the need for costly iterative searches. Through comprehensive
    experiments, we show that our method consistently outperforms
    state-of-the-art techniques, achieving high sparsity without significant
    performance losses.

\end{abstract}
