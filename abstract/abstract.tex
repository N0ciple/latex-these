

% abstract

\chapter*{Abstract}
Thanks to the miniaturisation of electronics, embedded devices have become
more and more ubiquitous, since the 2010s, realising various tasks all
around us. As their usage is developing, there is a growing demand for these
devices to process data and make complex decisions efficiently. Deep neural
networks are powerful tools to achieve this goal, however, these networks
are often too heavy and complex to fit on embedded devices. Thus, there is a
compelling need to devise methods to compress these large networks without
significantly compromising their efficacy. This PhD thesis introduces two
innovative methods, centred around the concept of pruning, aiming to
compress neural networks while ensuring minimal impact on their accuracy.

This PhD thesis first introduces a budget-aware method for compressing large
neural networks with weight reparametrisation and budget loss that does not
require fine-tuning. Traditional pruning methods often rely on post-training
saliency indicators to remove weights, disregarding the targeted pruning
rate. Our approach integrates a budget loss, driving the pruning process
towards a specific value during training, thereby achieving a joint
optimisation of topology and weights. By soft-pruning the smallest weights
using weight reparametrisation, our method significantly mitigates accuracy
degradation in comparison to traditional pruning techniques. We show the
effectiveness of our approach across various datasets and architectures.

This PhD thesis later focuses on the extraction of effective subnetworks
without weight training. Our goal is to identify the best subnetwork
topology in a large network without optimising its weights while still
delivering compelling performance. This is achieved using our novel
Arbitrarily Shifted Log Parametrisation, which serves as a differentiable
relaxation of discrete topology sampling, enabling the training of masks
that represent the probability of selection of the weights. Alongside, a
weight rescaling mechanism (referred to as Smart Rescale) is also
introduced, which allows enhancing the performance of the extracted
subnetworks as well as speeding up their training. Our proposed approach
also finds the optimal pruning rate after one training pass, thereby
circumventing computationally expensive gird-search and training across
various pruning rates. As shown through comprehensive experiments, our
method consistently outperforms closely related state-of-the-art techniques
and allows designing lightweight networks which can reach high sparsity
levels without significant loss in accuracy.

