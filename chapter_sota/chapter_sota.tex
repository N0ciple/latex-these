\chapter{Deep Neural Network Compression}
\label{chap:sota}


\localtableofcontents

\section{Introduction}
% TODO: relier avec ce que l'on a dit avant (context netatmo)

% region: intro
The development of neural networks and the enhancement of their performances has
been accompanied by a significant growth of their size, particularly in the
number of weights constituting them. In parallel, the evolution of these
networks has given rise to various applications, particularly embedded ones,
whose resources are highly constrained in terms of computing power, energy
consumption or memory footprint. Alongside the increase in the size of these
networks, compression techniques have been devised, in order to enable the use
of these algorithms in embedded applications or resource-contrained
environments.\\

This chapter focuses on these techniques and presents state-of-the-art neural
network compression methods, predominantly based on various modifications of
weights. First, we will do a brief overview of deep learning and their
evolution. Following this, we will examine fast convolution techniques, which
aim to accelerate the computation of convolutions in neural networks, thereby
reducing both the runtime and computational resources required. Thereafter, we
will delve into \ac{KD}, a process by which the knowledge of a larger, more
complex network (denoted the \emph{teacher}) is transferred to a smaller, more
efficient network (denoted the \emph{student}), enabling the latter to achieve
comparable performance with a reduced footprint. Subsequently, we will explore
architecture design methods that aims at producing more efficient and effective
networks. We will start with ad-hoc architectures, referred to here as
\emph{Efficient Architectures}. These networks are lightweight networks that
revolve around a core technique to reduce their size while preserving
performance as much as possible. Following this, we will discuss \ac{NAS}, a
method that automates the discovery of optimal network architectures tailored to
specific tasks or constraints, potentially leading to more compact and efficient
designs. Afterwards, we will focus on \emph{quantisation} and
\emph{binarisation} techniques, which aim to lower the numerical precision of
the weights and activations of the network in order to speed up the computation
and reduce the memory footprint. Lastly, we will consider neural network
pruning, a set of techniques that involve the removal of redundant or
insignificant connections and weights from the network, resulting in a sparser
and more computationally efficient model.\\

% endregion: intro

\section{Deep Learning overview}

Deep learning is a subfield of machine learning that focuses on the study of
\acp{DNN}. \acp{DNN} are a family of machine learning algorithms that have their
roots in \acp{ANN} that aim to learn a data representation from unstructured
data such as raw images, text or audio, in and end-to-end fashion. \acp{ANN}
were initially conceptualised based on the understanding of biological neural
networks present in the brain \cite{mcculloch1943logical,hebb2005organization}.
\citeauthor{rosenblatt1958perceptron} proposed in
\cite{rosenblatt1958perceptron} a theoretical model of a neuron, denoted the
\emph{perceptron}, which was capable of learning a linear decision boundary. The
perceptron model was later extended to multiple layers of neurons, giving rise
the \ac{MLP} \cite{rosenblatt1961principles,rumelhart1986learning}. A \acl{MLP}
is a type of artificial neural network that extends the concept of a
single-layer perceptron by including one or more hidden layers of neurons, with
each layer fully connected to the next, allowing the model to learn and
represent more complex, non-linear relationships in the input data. The term
\emph{deep} in the context of \acp{DNN} refers to the staking of multiple and
numerous layers within a neural network. A \ac{DNN} consist of layers, each
layer is an operation that takes information from the previous layers, process
it and then passes it through an activation function
\cite{glorot2011deep,DBLP:journals/pieee/LeCunBBH98,klambauer2017self} that
introduce non linearity, and finally this output is send to the next layer.
Among the most fundamental types of these layers are \emph{Convolutional} and
\emph{Dense} (or fully connected) layers. \\

\begin{figure}[htbp]
    \centering
    \subfloat[Convolutional Layer\label{fig:sota:conv_layer}]{
        \includegraphics[width=0.49\textwidth]{chapter_sota/assets/conv_layer.pdf}}
        \subfloat[Dense Layer\label{fig:sota:dense_layer}]{
        \includegraphics[width=0.49\textwidth]{chapter_sota/assets/dense_layer.pdf}}
        \caption{Conceptual representation of a convolutional layer and a dense
        layer. The convolutional layer (\cref{fig:sota:conv_layer}) takes a
        multi-channel input and produces a multi-channel output. Each coefficient of
        the output is computed by applying a convolution operation at a
        corresponding location in the input. The dense layer
        (\cref{fig:sota:dense_layer}) takes a vector input and produces a vector
        output. Each connection is represented by a weight in the weight matrix.}
    \label{fig:sota:layers}
    \end{figure}


Dense layers, often referred to as fully connected layers, are characterised by
neurons that connect to every neuron in the preceding layer. This characteristic
is central to \ac{MLP} networks, where these dense layers are stacked together.
Convolutional layers are typically used for image analysis tasks
\cite{DBLP:journals/pieee/LeCunBBH98}. They are composed of several kernels that
are convoluted with the input data. They are used for detecting edges, textures,
and patterns, and are often used as feature extractor. Typical neural network
targeted towards image processing are composed convolutional layers and thus
named \aclp{CNN}. In a such architecture, convolutional layers are stacked and
their output are usually fed into a fully-connected layer for further processing
\cite{DBLP:journals/corr/SimonyanZ14a,DBLP:conf/cvpr/HeZRS16,huang2017densely}.
Conceptual representations of these layers are shown in \cref{fig:sota:layers}
and a typical \ac{CNN} architecture is shown in
\cref{fig:sota:lenet5,fig:sota:vgg16}.\\

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.70\textwidth]{chapter_sota/assets/lenet.png}
    \caption{Architecture of LeNet-5, a \acl{CNN} used for handwritten digit
    recognition. Image taken from \cite{DBLP:journals/pieee/LeCunBBH98}}
    \label{fig:sota:lenet5}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{chapter_sota/assets/vgg16.png}
    \caption{Architecture of the VGG16 network introduced in
    \cite{DBLP:journals/corr/SimonyanZ14a}. Image taken from
    \cite{ferguson2017automatic}}
    \label{fig:sota:vgg16}
\end{figure}


The process of training a neural network involves adjusting the weights of the
network to minimize a loss function. The loss function is a measure of the error
between the predicted output of the network and the ground truth. Since neural
networks are differentiables, the weights can be adjusted using gradient-based
methods. The most common method is \acf{SGD}, derived from the Robbins–Monro
algorithm \cite{robbins1951stochastic}. In \ac{SGD}, the gradient of the loss
function is computed for a random subset of the data (a \emph{batch} or
\emph{mini-batch}), and the weights are shifted in the direction that decreases
the loss function. This direction is computed via the backpropagation algorithm
\cite{rumelhart1986learning} which calculates the gradient of the loss with
respect to each weight using the chain rule. \acl{SGD} is a prevalent method for
loss function optimisation in neural networks, however, numerous other methods
also exist. These include momentum-based strategies
\cite{sutskever2013importance}, adaptive learning rate approaches
\cite{zeiler2012adadelta} and moment estimation techniques
\cite{kingma2014adam}.\\


\section{Neural Networks Size and Achitecture Evolution}

The evolution of neural networks is characterized by a consistent increase in
their size and performance, alongside the introduction of new architectural
modifications to address limitations of their predecessors (see
\cref{fig:sota:net_sizes}). In 1998, LeNet-5 was developed for digit recognition
\cite{DBLP:journals/pieee/LeCunBBH98}, constituting a relatively simple network
with seven layers. Its size was significantly small compared to the contemporary
models. With the introduction of AlexNet \cite{DBLP:conf/nips/KrizhevskySH12} in
2012, the network size considerably grew, comprising more layers and neurons to
handle more complex tasks like large-scale image recognition. AlexNet tackled
the overfitting issue in LeNet-5 with the use of data augmentation and dropout
techniques.\\


The next advancement was the VGG networks family
\cite{DBLP:journals/corr/SimonyanZ14a} %, introduced in 2014 
which emphasized the importance of depth in neural networks. With up to 19
layers, VGG expanded on AlexNet's deep architecture, but the increased depth led
to vanishing gradient problems. In the same year, Google's Inception (or
GoogLeNet) \cite{DBLP:conf/cvpr/SzegedyLJSRAEVR15} was introduced, addressing
this issue with its novel inception modules, which allowed the network to learn
at varying scales and increased computational efficiency, without overly
increasing the network size.\\

%In 2015, 
Later, the ResNet models family was proposed in \cite{DBLP:conf/cvpr/HeZRS16},
which effectively tackled the vanishing gradient problem by introducing skip (or
shortcut) connections, allowing gradients to backpropagate directly through
several layers. These shortcut connections also allowed the network to grow in
depth up to 152 layers without a significant increase in computational cost.
However, a challenge remained with the constant need for careful design to
manage feature-map sizes.\\

In response, DenseNet \cite{huang2017densely} was proposed.% in 2017, 
It connects each layer to every other layer in a feed-forward fashion. By
reinforcing the propagation of features and gradients through the network,
DenseNet alleviated the vanishing-gradient problem and further improved upon the
network's ability to pass on information from earlier layers to later ones.
Thus, through these chronological advancements, neural networks not only grew in
size but also improved in performance, becoming more efficient and capable of
handling more complex tasks.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{chapter_sota/assets/network_sizes_normal.pdf}
    \caption{Networks size comparison. The \emph{x-axis} represents the number
    of \acp{FLOP} required to process a single image. The \emph{y-axis}
    represents the Top-1 accuracy on the ImageNet \cite{deng2009imagenet}
    dataset, and the size of the circles represents the number of parameters in
    the network. Numbers are taken from \cite{pytorch_vision}}
    \label{fig:sota:net_sizes}
\end{figure}

\section{Accelerating Computations in Neural Networks}
\label{sec:sota:fast_convolutions}


Neural networks, utilise two fundamental mathematical operations: convolution
and matrix multiplication. However, performing these operations can be
computationally demanding, particularly with large and complexe networks. This
can lead to significant processing times, posing a challenge for real-time or
resource-limited applications. To mitigate this, some research efforts have been
focused on developing techniques to speed up these operations. These strategies
range from optimizing the underlying algorithms to leveraging hardware
acceleration, with the objective of enhancing the speed and efficiency of neural
network computations.\\


The most popular algorithms for accelerating convolution operations rely on the
\ac{FFT}
\cite{DBLP:conf/nips/ChiJM20,DBLP:journals/npl/LinY19,DBLP:conf/pkdd/PrattWCZ17},
and leverage the Convolution Theorem. The Convolution Theorem states that the
convolution of two signals in the source domain is the product of the two
signals in the Fourier domain. This allows for a faster computation of the 2D
convolution by using the \ac{FFT} to compute the convolution in the frequency
domain \cite{oppenheim1997signals}.\\


Other algorithms focus on optimising matrix operations, such as the Strassen
algorithm \cite{strassen1969gaussian}, used in \cite{DBLP:conf/icann/CongX14}.
It is a fast method for matrix multiplication that reduces the computational
complexity from the standard $\mathcal{O}(n^{3})$ to approximately
$\mathcal{O}(n^{2.807})$ by recursively dividing the matrices of size $n$ into 4
submatrices of size $\frac{n}{2} \times \frac{n}{2}$, reorganising and combining
these multiplications to perform only 7 instead of 8 matrix multiplications. The
Strassen algorithm has later been refined by \citeauthor{coppersmith1987matrix},
who introduced the Coppersmith-Winograd algorithm \cite{coppersmith1987matrix}.
The latter brings down the complexity to $\mathcal{O}(n^{2.376})$. This
algorithm is used in various works, mostly targeted towards a specific \ac{FPGA}
architecture \cite{liu2018efficient,lu2018spwa,wang2020winonn}.\\


Using particular matrix structure also speed up the standard operations used in
a neural network. Fully connected layers can  effectively be accelerated by
forcing the use of particular matrix structure.
\citeauthor{DBLP:conf/iccv/ChengYFKCC15} devised a method where dense layers
standard operation is replaced by a circulant projection
\cite{DBLP:conf/iccv/ChengYFKCC15}. The circulant matrix can be stored in a
memory efficient way and can be furthuer speed up with \ac{FFT}. $C$ is an
example of a circulant matrix. Likewise, convolutional operations can be
accelerated thanks to Toeplitz matrices. A Toeplitz matrix, or diagonal-constant
matrix, has the unique characteristic of each descending diagonal from left to
right being constant. $T$ is an example of a Toeplitz matrix. This property is
particularly useful for convolutions, as they can be expressed as a
multiplication by a Toeplitz matrix \cite{gray2006toeplitz}. This algorithm has
been used in \cite{liao2019compressing}, with a focus on \ac{FPGA}
architectures. Note that the representation of a convolution as a product with a
Toeplitz matrix can further be accelerated by using the aforementioned
optimisations to the matrix multiplication algorithm.\\

\[
T = 
\begin{pmatrix}
a & b & c & d \\
e & a & b & c \\
f & e & a & b \\
g & f & e & a \\
\end{pmatrix},
\quad
C = 
\begin{pmatrix}
    a & b & c & d \\
    d & a & b & c \\
    c & d & a & b \\
    b & c & d & a
\end{pmatrix}
\]\\



The algorithms presented in this section offer significant acceleration in the
computation of convolution operations. In particular, the Coppersmith-Winograd
algorithm is used in various deep learning frameworks
\cite{DBLP:journals/corr/AbadiABBCCCDDDG16,DBLP:conf/nips/PaszkeGMLBCKLGA19} or
neural network \ac{GPU} libraries \cite{nvidia_cudnn} where the fastest
algorithm is automatically selected based on the tensor sizes and the hardware.
However, depending on the operand size, the execution speed can be bound to the
hardware and more precisely the memory thourghput and data access time rather
than the computation time.
\cite{DBLP:journals/pc/WhaleyPD01,DBLP:journals/cca/DrevetIS10}


\section{Teaching Paradigm}


\acf{KD} is a method that aims to transfer the knowledge of a large, complex and
accurate network referred to as the \emph{teacher} to a smaller, more efficient
one called the \emph{student}. The student is trained with a combination of the
main task loss as well as a supplementary supervision signal which is derived
from the feature maps of the teacher network at various depths.\\

Methods in this paradigm are mostly based on the seminal work of
\citeauthor{DBLP:journals/corr/HintonVD15} \cite{DBLP:journals/corr/HintonVD15}.
The latter seeks to train simple networks with \acl{KD} yielding better
performances compared to those trained from scratch. \ac{KD} relies on teacher
and student networks, where the logits of the former are used as an additional
supervision signal for the latter. When trained separately, the student network
can only rely on classification labels in order to learn its own data
representation while \ac{KD} relies on the logits of the teacher network which
provide more insight about the hidden representations.\\

Inspired by \ac{KD}, \cite{DBLP:journals/corr/RomeroBKCGB14} introduced FitNet,
a two-stage training algorithm, where an intermediate layer of the teacher is
chosen as a \emph{hint}\footnote{\emph{Hint} is the terminology used by
\citeauthor{DBLP:journals/corr/RomeroBKCGB14}
\cite{DBLP:journals/corr/RomeroBKCGB14} to denote a feature map used as a target
for the student network.} for an intermediate layer of the student. First, the
student is trained up to its selected layer to mimic the hint feature map. Then,
the whole student network is trained with standard \ac{KD} against the whole
teacher. In the first step, a regressor is needed in order to adapt the
dimensions of the feature map, which may differ from the teacher to the student
networks, as illustrated in \cref{fig:sota:kd_frameworks}.
\citeauthor{DBLP:conf/cvpr/YimJBK17} argue that the direct feature map matching
utilised by FitNets is overly restrictive. Drawing inspiration from the
techniques used in \cite{DBLP:journals/corr/GatysEB15a} for style transfer, they
propose an alternative method. In the context of style transfer, the Gram matrix
of the feature maps is employed to encapsulate the texture information of an
image. Adapting this approach, the method presented in
\cite{DBLP:conf/cvpr/YimJBK17} calculates the Gram matrix across multiple layers
feature maps. This computed Gram matrix, dubbed the Flow of Solution Procedure
(FSP) matrix, then serves as a \emph{hint} for the student network, guiding its
training process.\\

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{chapter_sota/assets/kd_frameworks.pdf}
    \caption{Overview of various knowledge distillation frameworks. From top to
    bottom, left to right: Deep Mutual Learning
    \cite{DBLP:conf/cvpr/ZhangXHL18}, FitNet
    \cite{DBLP:journals/corr/RomeroBKCGB14}, Attention Transfer
    \cite{DBLP:conf/iclr/ZagoruykoK17}, Teacher Assistant
    \cite{DBLP:conf/aaai/MirzadehFLLMG20} and Knowledge Distillation
    \cite{DBLP:journals/corr/HintonVD15}.}
    \label{fig:sota:kd_frameworks}
\end{figure}

In practice, handling full-dimensional feature maps is cumbersome. That is why,
in order to avoid this issue, \cite{DBLP:conf/iclr/ZagoruykoK17} use an
attention map generated by squashing the feature maps to a 2D map allowing for a
smaller 2D regressor to match attention map dimensions. Note that the
aforementioned knowledge transfer methods require teacher-student pairs and
assume that teachers are large trained models. \cite{DBLP:conf/cvpr/ZhangXHL18}
relax this assumption by proposing \emph{Deep Mutual Learning}, which enables a pool
of networks of different architectures to learn together, provided that they
have the same logit dimensions, and none of the models in the pool requires a
pretraining step. The uncertainty of each model is distilled to each other,
which creates additional knowledge.\\

In all the aforementioned methods, the efficacy of knowledge distillation, and
consequently, the final performance of the student network, is significantly
influenced by the disparity in size between the student and teacher networks.
This size discrepancy, when excessive, may cause the student network to
encounter difficulties in aligning with the teacher logits, thus preventing
optimal knowledge distillation. To tackle this issue,
\citeauthor{DBLP:conf/aaai/MirzadehFLLMG20} introduced the concept of
\emph{Teacher Assistant}: networks of intermediary dimensions aiming at bridging
the size gap between student and teacher \cite{DBLP:conf/aaai/MirzadehFLLMG20}.
The \ac{TA} approach proposes to ensure effective knowledge transfer
through a stepwise transfer of knowledge, starting from the teacher to the
Teaching Assistant, and finally to the student. This technique allows each model
to learn from a slightly simpler model than itself. Empirical evidence shows
that the \ac{TA} approach tends to outperform traditional one-step
distillation in various experiments and across different network architectures,
resulting in improved performances. However, it is important to note that it
does introduce additional computational overhead due to the necessity of
additional training steps for the \ac{TA}, and careful selection of
the size and number of \acp{TA}. These considerations underscore that
while the \ac{TA} strategy is effective in managing the size disparity
problem, it also adds complexity to the distillation process.\\


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{chapter_sota/assets/variational_info_distillation.pdf}
    \caption{Conceptual scheme of \cite{DBLP:conf/cvpr/AhnHDLD19}. The student
    network efficiently learns the main task while retaining high mutual information
    with the teacher network. The mutual information is maximised by learning to
    estimate the distribution of the activations in the teacher network, provoking
    the transfer of knowledge. Adapted from the original
    scheme found in \cite{DBLP:conf/cvpr/AhnHDLD19}}
    \label{fig:sota:vid_scheme}
\end{figure}

Other approaches that do not rely on direct feature map or logit matching have
been proposed. \cite{DBLP:conf/cvpr/AhnHDLD19} introduced \emph{Variational
Information Distillation}, which indirectly maximises the mutual information
between the student and the teacher. This is done by using \emph{variational
information maximisation} \cite{barber2004algorithm} to maximise a variational
lower bound of the mutual information, since directly maximising the latter is
intractable in practice (see \cref{fig:sota:vid_scheme}). Likewise,
\cite{DBLP:conf/eccv/PassalisT18} proposed a \emph{Probabilistic Knowledge Transfer}
method that does not match logits or feature maps, but rather represents the
latter as a probability distribution and minimises divergence between the two
(see \cref{fig:sota:pkt_scheme}).\\


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{chapter_sota/assets/pkt_diagram.pdf}
    \caption{Conceptual scheme of the
    Probabilistic Knowledge Transfer method. Both the student and the teacher
    feature maps are modelled using probability distributions. The divergence of the
    latter is minimised in order to transfer knowledge from the teacher to the
    student. Illustration taken from \cite{DBLP:conf/eccv/PassalisT18}.}
    \label{fig:sota:pkt_scheme}
\end{figure}

\section{Architecture Design}

\subsection{Building Blocks for Efficient Architecture Design}
\label{sec:sota:efficient_archi}
% region: depthwise separable convolutions

One of the initial strategies towards achieving efficiency in neural network
architectures is the use of depthwise separable convolutions. This technique,
utilised in \cite{howard2017mobilenets} and \cite{DBLP:conf/icml/TanL19},
separates the standard convolution operation into two distinct steps: a
depthwise convolution and a pointwise convolution (see
\cref{fig:sota:depthwise_conv_vs_standard_conv}). By decomposing the operations
in this manner, the computational complexity is markedly reduced while still
retaining the ability to capture spatial and channel-wise information. Consider
an input feature map with $C_\text{in}$ channels of arbitrary width and height
and $C_\text{out}$ convolution kernels of size $k\times k \times C_\text{in}$. A
standard convolution algorithm will need $C_\text{in} \times C_\text{out} \times
k \times k$ \ac{MAC} operations to produce a $1 \times 1 \times C_\text{out}$
element of the output feature map. In contrast, a depthwise separable
convolution algorithm will first apply a $k\times k \times 1$ convolution kernel
to the $C_\text{in}$ channels and then perform $C_\text{out}$ pointwise
convolutions with $1\times 1 \times C_\text{in}$ kernels to produce the same
$1\times 1 \times C_\text{out}$ element. This effectively reduces the number of
parameters to $C_\text{in} \times (C_\text{out} + k \times k)$, essentially
reducing the number of computations required to produce a $1 \times 1 \times
C_\text{out}$ element by a factor of\\

$$\displaystyle\frac{C_\text{out}\times k \times k}{C_\text{out} + k \times k}.$$\\

\begin{figure}[htbp]
\centering
\subfloat[Standard Convolution\label{fig:sota:standard_convolution}]{
    \includegraphics[width=0.70\textwidth]{chapter_sota/assets/standard_conv_scheme.pdf}}\\
    \vspace{1cm}
\subfloat[Depthwise Separable
    Convolution\label{fig:sota:depthwise_convolution}]{
    \includegraphics[width=0.70\textwidth]{chapter_sota/assets/depthwise_sep_conv_scheme.pdf}}
    \caption{Illustration schemes of the standard and depthwise separable
    convolution. The standard convolution uses $C_\text{out}$ kernels of size
    $k\times k \times C_\text{int}$. The depthwise separable convolution is
    split into two steps: \emph{(i)} a convolution with $C_\text{in}$ kernels
    of size $k \times k$ and \emph{(ii)} a convolution with $C_\text{out}$
    kernels of size $1\times 1 \times C_\text{int}$.
    Best viewed in colours.}
\label{fig:sota:depthwise_conv_vs_standard_conv}
\end{figure}

% endregion: depthwise separable convolutions

% region: fire module
An alternative approach for designing efficient architectures involves the
integration of \emph{fire modules}, as proposed in
\cite{DBLP:journals/corr/IandolaMAHDK16}. These modules aim to minimise
computational requirements by employing two distinct strategies: \emph{(i)}
diminishing the number of input channels supplied to the following conventional
$k\times k$ convolutions and \emph{(ii)} substituting a portion of the
resource-intensive $k\times k$ convolutions with pointwise convolutions, which
possess $k^2$ times fewer parameters. The initial strategy is applied within the
\emph{Squeeze Layer} of the \emph{fire module}, which functions to decrease the
number of input channels delivered to the \emph{Expand Layer}, subsequently
reducing the parameter count in the \emph{Expand Layer} kernels. The second
strategy is implemented in the \emph{Expand Layer}, where some $3\times3$
convolutions are replaced with $1\times1$ variants. Even though the $1\times1$
convolutions capture less spatial information, they are significantly less
computationally demanding.\\

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.70\textwidth]{chapter_sota/assets/fire_module.pdf}
    \caption{Illustration scheme of the fire module. The fire module is composed
    is composed of a \emph{squeeze layer} (pointwise convolution designed to
    reduce the number of channels fed to the following layer) and an
    \emph{expand layer} (convolution with mixed $1\times1$ and $3\times3$
    kernels. The $1\times1$ kernels replace some of the $3\times3$ kernels,
    being less computationally intensive.). Best viewed in colours.}
    \label{fig:sota:fire_module}
\end{figure}

% endregion: fire module

% region: shufflenet

Pushing the concept of depthwise separable convolutions further,
\cite{ZhangShuffleNet} introduces pointwise group convolutions and channel
shuffle operations to enhance efficiency while maintaining accuracy. Pointwise
group convolutions were initially introduced in
\cite{DBLP:conf/nips/KrizhevskySH12}, though their original purpose was not for
compression. Instead, group convolutions in \cite{DBLP:conf/nips/KrizhevskySH12}
were used to enable distributed training across multiple \acp{GPU} with limited
memory. However, ShuffleNet \cite{ZhangShuffleNet} leverages this concept for
network efficiency by dividing the input channels into groups and performing
convolutions on each group independently. This approach reduces the number of
operations and computational cost compared to traditional convolutions. To
counteract the potential loss of expressive power caused by the separation of
channels into groups, ShuffleNet incorporates \emph{channel shuffle operations}
as shown in \cref{fig:sota:shuffle_net}. This technique allows for information
exchange between groups, effectively maintaining accuracy by ensuring that
different groups can capture diverse features in the input.\\

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.70\textwidth]{chapter_sota/assets/group_conv_and_channel_shuffling.pdf}
    \caption{Illustration scheme of grouped convolution with channel shuffling.
    Each filter only acts on a subset of the input tensor (here represented by a
    matching colour). The channels of the yielded tensor are shuffled to ensure
    the subsequent groups can access information from all the previous groups.
    Best viewed in colours.}
    \label{fig:sota:shuffle_net}
\end{figure}

Following ShuffleNet, CondenseNet was introduced in \cite{huang2018condensenet},
incorporating learned group convolutions to further enhance efficiency. Unlike
the predefined group convolutions in ShuffleNet, CondenseNet learns which
channels should be grouped together, enabling the network to adapt its structure
for a specific task. This results in better utilisation of network capacity and
reduces redundancy. CondenseNet leverages the DenseNet architecture
\cite{huang2017densely} to further improve performance. Thanks to the densely
connected architecture, features discarded in any layer can still be recovered
in subsequent ones.\\

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.3\textwidth]{chapter_sota/assets/channel_split.pdf}
    \caption{Illustration scheme of the path taken by the feature maps after the
    channel split block. Adapted from the original scheme found in \cite{MaShuffleNetV2}.}
    \label{fig:sota:channel_split}
\end{figure}

Building on the success of ShuffleNet, ShuffleNetV2 was introduced in
\cite{MaShuffleNetV2}, focusing on enhancing network efficiency through the
combination of strided convolution and channel split. Strided convolution helps
reduce the spatial dimensions of feature maps, thereby reducing the computation
cost. The Channel Split technique efficiently processes the input feature maps
while maintaining the expressive power of the architecture. Channel Split works
by dividing the input feature maps into two equal parts. One part is passed
through the main branch of the ShuffleNet unit, while the other part is sent
through the identity branch, which leaves its input unchanged. In the main
branch, a sequence of pointwise and $3\times 3$ convolutions are performed.
After both the main branch and the identity branch complete their respective
operations, the two parts are concatenated along the channel dimension and the
channels are shuffled. Finally, the output feature maps are passed to the next
ShuffleNet unit in the network. This process is represented in
\cref{fig:sota:channel_split}. This approach balances computational efficiency
with the expressive capacity of the model.\\
% endregion: shufflenet

% region: mobilenetv2
Depthwise Separable Convolutions were employed and improved in
\cite{howard2017mobilenets}. \citeauthor{DongMobileNetV2} introduced skip
connections and residual blocks into the MobileNet architecture, initially
proposed in \cite{DBLP:conf/cvpr/HeZRS16}. They also introduced the concept of
inverted residuals and linear bottlenecks. In conventional residual blocks, the
input is first compressed, then expanded, and finally compressed again after
being added to the original input. With inverted residual bottlenecks, on the
other hand, this process is reversed: the input is first expanded, then a
depthwise separable convolution is applied, and finally, it's compressed again.
In this architecture, the skip connections link the feature maps of smaller
size, instead of the larger ones. This allows for a more memory-efficient
architecture. The standard residual blocks and the inverted residual blocks are
shown in \cref{fig:sota:inverted_vs_residual_blocks}. The linear bottlenecks, on
the other hand, are convolutions without non-linear activation functions like
\ac{ReLU}. This takes advantage of the property that high-dimensional feature
maps can be embedded in a lower-dimensional manifold. To do this, it is
necessary to use linear transformations without an activation function, which
could potentially destroy information.\\

\begin{figure}[htbp]
    \centering
    \subfloat[Standard Residual Block\label{fig:sota:residual_block}]{%
        \includegraphics[width=0.49\textwidth]{chapter_sota/assets/mobilenet_v2_residual.png}}
    \subfloat[Inverted Residual Block\label{fig:sota:inverted_residual_block}]{%
        \includegraphics[width=0.49\textwidth]{chapter_sota/assets/mobilenet_v2_inverted_residual.pdf}}
    \caption{Illustration scheme of the residual block and the inverted residual
    block. Note that on the inverted residual block, the feature maps with the lower
    channel count are the ones connected via the skip connection, whereas it is the
    opposite on the standard residual block. Diagonally hatched layers do not use
    non-linearities. The grey colour indicates the beginning of the next block. Both
    illustrations are taken from \cite{DongMobileNetV2}. Best viewed in colours.}
    \label{fig:sota:inverted_vs_residual_blocks}
\end{figure}

% endregion: mobilenetv2

% region: mobilenetv3
Advancing from MobileNet and MobileNetV2, its third iteration
\cite{DBLP:conf/iccv/HowardPALSCWCTC19} incorporated \ac{SE} modules initially
introduced in \cite{DBLP:conf/cvpr/HuSS18}. These modules adaptively recalibrate
channel-wise feature responses, amplifying important features and suppressing
less relevant ones. The \ac{SE} module (represented in
\cref{fig:sota:se_module}) performs \emph{squeeze} and \emph{excitation}
operations. The squeeze operation uses global average pooling to create a
channel descriptor that summarises the spatial information for each channel. The
excitation operation uses this descriptor to learn non-linear interactions
between channels through two fully connected layers. The output of this
mini-network are per-channel modulation weights that recalibrate the original
feature maps, scaling or "exciting" them by these weights.\\

% endregion: mobilenetv3

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.70\textwidth]{chapter_sota/assets/SE_module.pdf}
    \caption{Illustration scheme of the \acf{SE} module. The original feature
    map is \emph{squeezed} into a channel descriptor through global average
    pooling. This descriptor is then used to learn the interdependencies between
    the channels through two fully connected layers. The output is then
    multiplied layerwise with the original feature map (\emph{excitation}). Best
    viewed in colours.}
    \label{fig:sota:se_module}
\end{figure}


% region: transition paragraph
The architectures we have just reviewed revolve around specific key techniques
such as depthwise separable convolutions, fire modules, channel shuffling, and
\ac{SE} modules, among others. These architectures, while highly efficient, are
manually crafted and require a significant degree of human expertise, intuition,
and time to develop, optimise, and fine-tune. The manual design of these
architectures often relies on a deep understanding of the tasks at hand, the
data they will process, and the constraints of the environment in which they
will operate. However, the process of designing these efficient architectures
can be automated, which is the subject of the next section. Sizes and
performance of networks architecture detailed in this section can be compared to
standard architecture sizes on \cref{fig:sota:net_sizes_std_eff}.\\

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.70\textwidth]{chapter_sota/assets/network_sizes_normal_eff.pdf}
    \caption{\Cref{fig:sota:net_sizes} updated with the size and performance of the efficient
    architectures detailed in \cref{sec:sota:efficient_archi}}
    \label{fig:sota:net_sizes_std_eff}
\end{figure}




% endregion: transition paragraph

\subsection{Automatic Architecture Design Through Neural Architecture Search}
\label{sec:sota:nas} 

\acf{NAS} is a method that automates the discovery of neural network
architectures, potentially leading to more compact, efficient designs and
reducing the need for manual intervention. Although \ac{NAS} might not
explicitly aim at producing lightweight architectures, it can still yield
designs that strike a good balance between performance and computational cost
\cite{DBLP:conf/cvpr/TanCPVSHL19,DBLP:conf/icml/TanL19}. By using automated
methods to search for optimal architectures, it is possible to further enhance
the efficiency of neural networks, opening up new possibilities for their
deployment in resource-constrained environments. \ac{NAS} has emerged as an
essential paradigm, aiming to automate the traditionally manual and
labour-intensive process of designing efficient neural networks
\cite{DBLP:journals/corr/MiikkulainenLMR17}. Early network architectures were
indeed entirely handcrafted, requiring significant human effort and expertise.
However, these manual methods are being replaced by \ac{NAS} techniques, which
seek to automatically determine the optimal network structure given a training
set \cite{DBLP:journals/corr/abs-2301-08727,elsken2019neural}.\\

The performance and efficiency of \ac{NAS} are fundamentally determined by two
key aspects: the \emph{search space} and the \emph{search strategy}. The search
space, as the term suggests, defines the set of all possible architectures that
can be discovered by the \ac{NAS} algorithm. It could be as broad as all
possible configurations of a certain type of network, such as \acp{CNN}, or as
narrow as different arrangements of a specific set of layers
\cite{DBLP:conf/cvpr/LiuCSAHY019}. The search strategy, on the other hand,
determines how the \ac{NAS} algorithm navigates through this search space in
order to optimise its given objective. This could involve gradient-based
strategies \cite{DBLP:conf/iclr/LiuSY19,DBLP:conf/iclr/XuX0CQ0X20}, or
stochastic methods, such as evolutionary algorithms and reinforcement learning
strategies \cite{DBLP:conf/iclr/ZophL17,DBLP:conf/icml/RealMSSSTLK17}. The
choice of search space and search strategy significantly influences the ability
of \ac{NAS} to discover effective and efficient architectures, and is thus a
critical aspect of NAS research. In the following paragraphs, we will delve
deeper into some of these strategies and their impact on the field of
\ac{NAS}.\\



The search space is a critical aspect of \ac{NAS} as it bounds the possibilities
of architectures and significantly influences the outcome of the search. The
search space could be as broad as all possible configurations of a certain
network type or as specific as various arrangements of a predefined set of
layers or blocks. For instance, \cite{DBLP:conf/iclr/ZophL17} define their
search space as a set of repeatable sub-structures composed of basic layers
(convolution layers, fully connected layers, \ac{batch norm} layers, etc...)
often called \emph{cells} that are stacked to form the final architecture, while
\cite{DBLP:conf/iclr/XieZLL19} design their search space based on the
connectivity patterns between network blocks. \cite{DBLP:conf/iclr/LiuSY19}
propose a continuous search space where the architecture is parameterized as a
differentiable function, allowing for efficient search using gradient-based
methods. Hierarchical search spaces, on the other hand, offer a strategic
approach to navigate the complexity of the architecture search in \ac{NAS}
\cite{DBLP:conf/cvpr/LiuCSAHY019,DBLP:conf/cvpr/TanCPVSHL19}. In such a setup,
the architecture is divided into several levels of hierarchy, with each level
searched independently. This structure enables a more systematic and organized
exploration of the search space, allowing the algorithm to uncover useful
patterns and configurations at different levels of the network. The EfficientNet
models are exemplary of innovative architecture search strategies
\cite{DBLP:conf/icml/TanL19}. This series utilizes both \ac{NAS} and
\emph{compound scaling}. A baseline, EfficientNet-B0, was developed through
multi-objective \ac{NAS}, optimizing both accuracy and \acp{FLOP}. Subsequently, a
compound scaling method was applied to this baseline, uniformly scaling depth,
width, and resolution via a \emph{compound coefficient} $\phi$. This approach
yielded a series of progressively larger EfficientNet models, whose performances
are shown in \ref{fig:sota:efficientnet_perfs}.\\\


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.70\textwidth]{chapter_sota/assets/efficientnet_perfs_overview.pdf}   
    \caption{ImageNet top-1 accuracy vs model size (in millions of parameters).
    The EfficientNet family of models significantly outperforms other models of
    similar size, obtained either by \ac{NAS} or manual design. This graph is
    taken from \cite{DBLP:conf/icml/TanL19}.
    }
    \label{fig:sota:efficientnet_perfs}
\end{figure}


The search strategy is another major component of \ac{NAS}, dictating how the
algorithm explores the search space to find the optimal architecture. A wide
range of search strategies have been proposed. Evolutionary algorithms
\cite{DBLP:conf/icml/RealMSSSTLK17} use principles of natural evolution such as
mutation, crossover, and selection to explore the search space. Despite their
potential to find high-quality solutions, these methods often require
substantial computational resources due to the large number of evaluations
needed. Reinforcement Learning-based methods \cite{DBLP:conf/iclr/ZophL17}
employ a policy network to generate architectures and a reward signal, typically
validation accuracy, to guide the search. While reinforcement learning methods
can effectively navigate large search spaces, their success heavily depends on
the quality of the reward signal. Gradient-based methods like
\cite{DBLP:conf/iclr/LiuSY19,DBLP:conf/iclr/XuX0CQ0X20} make the search space
continuous and use gradient descent for optimization, which enables efficient
exploration of the search space but requires careful regularization to prevent
overfitting. \cite{DBLP:conf/nips/BergstraBBK11} uses Bayesian optimization to
build a probabilistic model of the objective function and uses it to select
promising architectures, balancing exploitation and exploration. This method can
be sample-efficient but might struggle with high-dimensional spaces. These
diverse strategies offer multiple paths to navigate the complex landscape of
architecture search, each with its unique trade-offs between efficiency,
effectiveness, and computational demands.\\


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.70\textwidth]{chapter_sota/assets/network_sizes_normal_eff_nas.pdf}
    \caption{\Cref{fig:sota:net_sizes_std_eff} updated with the size and
    performance of architectures detailed in \cref{sec:sota:nas}}
    \label{fig:sota:net_sizes_std_eff_nas}
\end{figure}


\section{Refining and Optimising and Existing Neural Network}

% TODO: paragraphe d'introduction sur les méthodes de rafinement d'architectures

\subsection{Quantisation and binarisation}

Quantisation is the process where a broad range of input values (generally
continuous) are converted into a smaller, specific set of output values, which
often consists of a limited number of elements. Historically, the training of
neural networks has largely relied on the use of \ac{FP32}. FP32 has been the
default choice due to its wide support across various hardware platforms and
software libraries, which has made it a practical and convenient choice for the
majority of machine learning tasks \cite{sze2017efficient}. However, using
\acl{FP32} is not always necessary, and it possible to quantise a neural network
to use lower precision values, while maitaining compelling performances. \\

Quantising a neural network has been proposed as early as the 1990s
\cite{balzer1991weight,fiesler1990weight}. This later regain traction as
\citeauthor{37631} levraged Single Input Multiple Data instructions (SIMD) of
x86 CPU to speedup the fixed-point 8-bit operations \cite{37631}.
\citeauthor{gupta2015deep} used uniform quantisation with fixed-point 16-bit
with stochastic rounding to train neural networks. Quantisation has also been
applied together with K-means clustering \cite{steinhaus1956division}.
\cite{DBLP:journals/corr/HanMD15} uses K-means clustering to iteratively compute
a lookup table or \emph{code book} for the weights. This code book is later
further compressed using Huffman coding \cite{huffman1952method}. Note that this
method is mostly useful for storage, but for training or inference, the weights
need to be decompressed and their original value fetch in the code book before
being used.\\

Logarithmic quantisation provide compelling alternatives to uniform
quantisation. On the one hand, logarithmic quantisation enable to quantise
weights with a larger dynamic range compared to uniform or linear quantisation.
On the other hand, multiplication can be conventiently represented as an
inexpensive bit shift operation if operands are properly represented in the
logarithmic base. This is particularly beneficial for \ac{FPGA} implementations
\cite{alemdar2017ternary}. To levrage this potential speedup,
\cite{DBLP:journals/corr/LinCMB15} forced the weight representation to be a
power of two. \cite{DBLP:conf/iclr/ZhouYGXC17} improved this technique by
applying it iteratively.\\

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.40\textwidth,trim=5cm 9cm 5cm 9cm, clip]{chapter_sota/assets/binarised_kernels.pdf}
    \caption{Example of binarised kernels and activations in a convolutional
    layer. The kernels are taken from the first layer of a \ac{CNN} trained on CIFAR-10.
    Image taken from \cite{DBLP:conf/nips/HubaraCSEB16}.}
    \label{fig:sota:binarised_kernels}
\end{figure}

A more extreme version of the quantisation has been proposed in
\cite{courbariaux2015binaryconnect}, where the weight values are either $-1$ or
$+1$. The concept of minimising the bit-width of weights to a bare minimum is
called \emph{binarisation}. This has been further developed in
\cite{DBLP:conf/nips/HubaraCSEB16}, where the authors proposed a method to
binarise both weights and activations (see example of binarised kernels in
\cref{fig:sota:binarised_kernels}). DoReFa-Net \cite{zhou2016dorefa} build upon
the success of binarised neural network and introduced the stochastic 8-bit
quantisation of the gradients during the backward pass to accelerate both
training and inference.\\

Quantisation methods that quantise weights or activations after the training are
called \ac{PTQ} methods. Quantising an already existing network is a widely used
technique in the most famous deep learning framework
\cite{ncnn,qnnpack,snapdragon,tensorrt}. Because they quantise the weight after
the training, \ac{PTQ} methods often introduce an irreversible information loss
and  a performance drop that need to be compensated for
\cite{DBLP:journals/ijon/LiangGWSZ21}. To solve for this issue, several works
proposed proposed to take into account the quantisation effect during the
training. These methods are called \ac{QAT} methods. BinaryConnect
\cite{courbariaux2015binaryconnect} use a variant of Bayesian inferencec called
Expectation Back Probagation (EBP)
\cite{DBLP:journals/corr/ChengSML15,DBLP:conf/nips/SoudryHM14}. Another
binarisation method, uses \acl{STE} \cite{DBLP:journals/corr/BengioLC13} to
bypass the binarisation function in the backward pass
\cite{DBLP:conf/nips/HubaraCSEB16}. \cite{DBLP:conf/cvpr/JacobKCZTHAK18} also
uses \ac{STE} together with \emph{fake quantisation} nodes for 8-bit
quantisation (see \cref{fig:sota:fake_quant}). The fake quantisation nodes are
injected inside the computaion graph and simulates the effect of quantisation in
the forward pass.

\begin{figure}[htbp]
\centering
\subfloat[Fake quantisation inference\label{fig:sota:fake_quant_inference}]{%
    \includegraphics[height=6cm]{chapter_sota/assets/fake_quant_inference.pdf}}
\subfloat[fake quantisation training\label{fig:sota:fake_quant_training}]{%
    \includegraphics[height=6cm]{chapter_sota/assets/fake_quant_training.pdf}}
\caption{Fake quantisation nodes (\emph{fake quant.}) are included in the
computation graph of \cref{fig:sota:fake_quant_training}, whereas
\cref{fig:sota:fake_quant_inference} represent the computaion graph used during
inference. During the inference, weights are stored in \texttt{uint8} format,
whereas the bias are not, because their computational overhead is
negligible.\cite{DBLP:conf/cvpr/JacobKCZTHAK18}. Both illustrations are adapted
from \cite{DBLP:conf/cvpr/JacobKCZTHAK18}.}
\label{fig:sota:fake_quant}
\end{figure}

\subsection{Pruning}
\label{sec:sota:pruning}

% TODO: segmentation pruning structuré et non structuré

% ==============================================================================
