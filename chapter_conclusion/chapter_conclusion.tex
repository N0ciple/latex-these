\chapter{Conclusion and Perspectives}

\section{Summary of contributions}

In this thesis, we addressed the issue of \aclp{DNN} compression, specifically
from the perspective of pruning, and in particular, we focused on the problem of
performance drop after pruning. We proposed several solutions to address this
issue and ultimately questioned the very necessity of training the weights. We
summarised our contributions in the following paragraphs.\\

\noindent \textbf{Budget-aware pruning with weight reparametrisation.} Pruning a
network post-training introduces a performance drop that needs to be compensated
for with fine-tuning. In \cref{chap:chapter1} we propose a budget-aware pruning
method based on a weight reparametrisation. Respecting a budget throughout
training allows for joint optimisation of the weights and the topology.
Moreover, by controlling the number of parameters that will remain, it
encourages the network not to use more capacity and therefore weights than what
will be allowed once pruning is enforced.  To reach this goal, we introduce in
\cref{chap:chapter1} two main components that work together. On the one hand, a
budget regularisation loss that computes the current weight budget at each
training step, guiding the optimisation process to adhere to it. On the other
hand, a weight reparametrisation that embeds the saliency of the weights in
their expression and thereby soft-prune them during training. Both components
are based on our reparametrisation function that acts as a surrogate $\ell_0$
norm and have been carefully designed to be differentiable and numerically
stable.\\

We validated our approach by comparing our method against magnitude pruning with
and without fine-tuning on various datasets and network architectures. Our
method performs consistently better than magnitude pruning without fine-tuning
and, for almost all tested pruning rates, better than magnitude pruning with
fine-tuning. We also validated the relevance of each component of our method
individually in a set of comparative experiments. Finally, we provided
experimental results to discuss and support the choice of the mixing coefficient
and tested our method on trained and pruned initialisation to show the
importance of budget enforcement and weight reparametrisation, even on already
pruned networks when they undergo fine-tuning.\\

\noindent \textbf{Pruning without weight training with stochastic sampling.}
When it comes to estimating the saliency of weights, the general approach is to
derive an indicator based on their value, such as magnitude pruning which
considers the absolute value of the weight as its saliency. However, these
approaches, by design, cannot treat differently two connections with the same
weight value. In \cref{chap:chapter2}, we proposed a new stochastic approach to
extract lightweight subnetworks from a large untrained network. This approach
estimates the importance of a weight based on trained masks which are auxiliary
variables that represent their associated weight saliency and are consequently
not bound to the value of the weights. Furthermore, to also tackle the
aforementioned issue with the necessity to fine-tune pruned networks, the method
detailed in \cref{chap:chapter2} does not require any weight training and relies
purely on topology selection through the optimisation of the auxiliary masks.
This method works by stochastically sampling topologies from a large untrained
network, based on the value of the masks, interpreted as probabilities of
selection of the corresponding weight. These sampled topologies are evaluated to
eventually identify a subnetwork with compelling performances. The subnetwork is
extracted by pruning the weights of the large network identified as redundant
from the larger network, with no performance drop. To achieve this, we
introduced two components called \acf{ASLP} and \acf{SR}. The former is a
computationally efficient and numerical stable technique that relies on \acl{GS}
to train the masks in a stochastic context. The former is an efficient
learnt-based weight rescaling mechanism that allows the network to rescale the
weight distributions in order to mitigate the disruption of the weight
distribution statistics caused by the pruning. We also introduce a thresholding
strategy responsible for pruning the weights, that allows to effectively
\emph{freeze} the topology.\\

We validated our approach by comparing our method against other state-of-the-art
methods on various datasets and network architectures. Our method performs
better than those other methods in most tested scenarios, offering higher
accuracy. We also provided experimental results to validate the relevance of our
\ac{SR} mechanism and thresholding strategy, support our choice of learning rate
and finally, show that our method is robust to modification of the pruning rate
post-training. Finally, our code has been made publicly available \footnote{Code
available at: \url{https://github.com/N0ciple/ASLP}} and contains the
instructions to reproduce our results as well as a reimplementation of the
state-of-the-art method we benchmark against in PyTorch.\\ 

\section{Perspectives}

In this section, we discuss the perspectives and future works that could be
undertaken to improve the methods we proposed in this thesis as well as push
forward the findings we made.\\

\noindent \textbf{Experimental validation on larger datasets and architectures.}
In our experiments, we chose to focus on results reliability and therefore we
chose to run every configuration for every experiment at least 5 times to
average the results and provide their standard deviation. This choice was made
to avoid drawing conclusions based on a single run that could be an outlier.
However, this choice comes at the cost of computational time and resources,
thereby limiting the scale of datasets and architectures we could evaluate.\\

Futur works and development efforts could target the evaluation of our method on
larger networks and datasets, namely the ResNet-50 architecture
\cite{DBLP:conf/cvpr/HeZRS16}, Vision Transformers
\cite{DBLP:conf/iclr/DosovitskiyB0WZ21}, both in combination with the ImageNet
dataset \cite{DBLP:journals/ijcv/RussakovskyDSKS15}. A larger dataset like
ImageNet would allow to sample more topologies and therefore explore the
topology space more thoroughly.\\

\noindent \textbf{Structured Pruning.} The methods introduced in
\cref{chap:chapter1,chap:chapter2} are unstructured pruning methods, meaning
that they prune weights individually which is a flexible approach that allows to
reach high pruning rates. However, the speedup obtained by unstructured pruning
is not straightforward and could necessitate additional optimisations. On the
other hand, structured pruning methods, which prune weights in groups, yield
networks with lower pruning rates but with a regular structure. This regularity
can be exploited to obtain a more straightforward speedup in the most popular
Deep Learning frameworks
\cite{DBLP:conf/nips/PaszkeGMLBCKLGA19,DBLP:journals/corr/AbadiABBCCCDDDG16}.\\

Our method, \acf{ASLP}, could benefit from a structured pruning approach. In
addition to the aforementioned network regularity, using a structured approach
could allow to reduce the number of masks to train. Instead of training a mask
per weight, it is possible to train a mask per group of weights. This could lead
to significant memory savings and speedups during training since the sampling
operation takes a heavy toll on the \ac{GPU}. Our preliminary works on a
semi-structured approach, where we start by pruning the network with a
structured approach and then perform an unstructured pruning step afterwards,
limits the sampling: we only sample the weights that are not pruned by the
structured part. This approach is promising since it can reduce on average the
number of masks to sample. However, the theoretical sampling speedup is not
observed in practice due to memory latency caused by partial access to the
masks. A careful reimplementation of the mask partial selection and sampling
logic could resolve this issue and allow for faster sampling.\\


\noindent \textbf{Controlling mask magnitude.} In \cref{chap:chapter2}, we used
a learning rate value of 50 that is several orders of magnitude higher than
standard learning rates used in baselines training \cite{nvidia-baselines}. This
choice is motivated and explained in \cref{sec:chap2:impact_learning_rate}.
However, this high learning rate together with vanishingly small gradients as
masks move away from the origin (as explained in
\cref{sec:chap2:stochastic-sampling}) can lead to masks being stuck at their
high or low value and therefore being effectively frozen. \\

Adding a regularisation term to the loss function that penalises masks with
extreme values, or any other mechanism that can limit the magnitude of the masks
could help to mitigate this issue and prevent a mask from being frozen. Our
preliminary experiments with naive regularisation loss show improved results in
the aforementioned semi-supervised setup.\\


\noindent \textbf{Better initialisation scheme.} The \ac{ASLP} method introduced
in \cref{chap:chapter2} extracts a lightweight and effective neural network from
a large untrained one. The weights of the large network are initialised with
state-of-the-art methods such as Kaiming initialisation
\cite{DBLP:conf/iccv/HeZRS15} and are not modified. However, these
initialisations are designed with weight training in mind and might not be
optimal for the \ac{ASLP} method which does not train the weights.\\

A better initialisation scheme could be designed to improve the performance of
the \ac{ASLP} method. This initialisation scheme could be inspired by trained
weights distributions and could be designed to be more robust to the pruning and
sampling operation.\\

\noindent \textbf{Training through pruning.} \ac{ASLP} and experiments conducted
in \cref{chap:chapter2} showed that it is possible to achieve compelling
performances without training the weights. This raises the question of the very
necessity of training the weights and opens the way for new research directions
that investigate the possibility of training a network through pruning.
Furthermore, in this context, the word \emph{training} is to be understood
\emph{lato sensu} and could include any strategy that selects a topology, not
necessarily strategies that rely on gradient-based mask training as we proposed.